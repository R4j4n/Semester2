{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **linear regression** is a linear approximation of a causal relationship between two or more (quantitative) variables.\n",
    "\n",
    "**Independent variable (predictor) : X1, X2, X3 ….. Xn**\n",
    "\n",
    "**Dependent variable (predictand) : Y**\n",
    "\n",
    "$$\n",
    "y=b_0+b_1\\star x_1+b_2\\star x_2+...+b_n\\star x_n\n",
    "$$\n",
    "\n",
    "- y - dependent variable\n",
    "- x₁, x₂, .… xₙ - independent variables\n",
    "- b₀ - intercept, the value of y when independent variables are zero\n",
    "- b₁, b₂, .… bₙ - regression coefficient of independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures \n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils._bunch.Bunch'>\n",
      "<class 'numpy.ndarray'>\n",
      "(20640, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_0  feature_1  target\n",
       "0     8.3252       41.0   4.526\n",
       "1     8.3014       21.0   3.585\n",
       "2     7.2574       52.0   3.521\n",
       "3     5.6431       52.0   3.413\n",
       "4     3.8462       52.0   3.422"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fetch data \n",
    "housing = fetch_california_housing()\n",
    "print(type(housing))\n",
    "print(type(housing.target))\n",
    "\n",
    "data = housing.data[:,0:2]\n",
    "target = housing.target\n",
    "\n",
    "# create dataframe \n",
    "df = pd.DataFrame(data, columns=[\"feature_0\",\"feature_1\"])\n",
    "df[\"target\"] = target\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14448, 2)\n",
      "(14448,)\n"
     ]
    }
   ],
   "source": [
    "X = df[[\"feature_0\",\"feature_1\"]]\n",
    "y = df[\"target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple Linear regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression = LinearRegression()\n",
    "model = regression.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R-squared (Coefficient of Determination)**\n",
    "\n",
    "R-squared is a statistical measure of how close the data are to the fitted regression line. It is also known as the **coefficient of determination**, or the coefficient of multiple determination for multiple regression.\n",
    "\n",
    "$$\n",
    "R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}\n",
    "$$\n",
    "\n",
    "- $SS_{res}$ is the sum of squares of the residual errors.\n",
    "- -$SS_{tot}$ is the total sum of squares (proportional to the variance of the data):\n",
    "\n",
    "$$\n",
    "SS_{res} = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "SS_{tot} = \\sum_{i=1}^n (y_i - \\bar{y})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 for train: 0.5092773054476163\n",
      "R2 for test: 0.5087405786681392\n"
     ]
    }
   ],
   "source": [
    "r2_train = r2_score(y_train,y_pred_train)\n",
    "print(f\"R2 for train: {r2_train}\")\n",
    "\n",
    "r2_test = r2_score(y_test,y_pred_test)\n",
    "print(f\"R2 for test: {r2_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RMSE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for train:0.7777053315889283\n",
      "RMSE for test:0.7746422931023739\n"
     ]
    }
   ],
   "source": [
    "rmse_train = math.sqrt(mean_absolute_error(y_train, y_pred_train))\n",
    "print(f\"RMSE for train:{rmse_train}\")\n",
    "      \n",
    "rmse_test = math.sqrt(mean_absolute_error(y_test, y_pred_test))\n",
    "print(f\"RMSE for test:{rmse_test}\")\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept : -0.10299578449607161\n",
      "Coeff : [0.43176259 0.01743944]\n"
     ]
    }
   ],
   "source": [
    "# get the intercept \n",
    "print(f\"Intercept : {model.intercept_}\")\n",
    "\n",
    "# get model coefficent \n",
    "print(f\"Coeff : {model.coef_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- High R² on training and low on test data: Likely overfitting.\n",
    "- Low R² on both training and test data: Likely underfitting.\n",
    "- Low RMSE on training and high on test data: Likely overfitting.\n",
    "- High RMSE on both training and test data: Likely underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Interaction effect means that two or more features/variables combined have a significantly larger effect on a feature as compared to the sum of the individual variables alone. \n",
    "\n",
    "- This effect is important to understand in regression as we try to study the effect of several variables on a single response variable.\n",
    "\n",
    "$$\n",
    "\\mathbf{y}=\\mathbf{\\beta}_0+\\mathbf{\\beta}_1*\\mathbf{X}_1+\\mathbf{\\beta}_2*\\mathbf{X}_2+\\mathbf{\\beta}_3*\\mathbf{X}_2*\\mathbf{X}_1+\\epsilon \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The degree parameter determines the maximum number of features to create interaction\n",
    "# We set interaction_only = Ture  to have only the interaction terms, not the polynomial features (those raised to an exponent).\n",
    "\n",
    "interaction = PolynomialFeatures(degree=3, include_bias=False, interaction_only=True)\n",
    "features_interaction = interaction.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14448, 3)\n",
      "[  4.1312  35.     144.592 ]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features_interaction, y , test_size= 0.3, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 for train: 0.5098224693975545\n",
      "R2 for test: 0.5094987356605216\n"
     ]
    }
   ],
   "source": [
    "regression = LinearRegression()\n",
    "model = regression.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "r2_train = r2_score(y_train,y_pred_train)\n",
    "print(f\"R2 for train: {r2_train}\")\n",
    "\n",
    "r2_test = r2_score(y_test,y_pred_test)\n",
    "print(f\"R2 for test: {r2_test}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general form of a polynomial regression equation is:\n",
    "$$y=\\beta_0+\\beta_1x+\\beta_2x^2+\\beta_3x^3+...+\\beta_1x^n$$\n",
    "\n",
    "Where \n",
    "- $n$ represents the highest power of the independent variable in the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "take_any_data -> apply PolynomialFeatures -> train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.32520000e+00, 4.10000000e+01, 6.93089550e+01, 3.41333200e+02,\n",
       "       1.68100000e+03, 5.77010912e+02, 2.84166716e+03, 1.39946612e+04,\n",
       "       6.89210000e+04])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
    "features_polynomial = poly.fit_transform(X)\n",
    "features_polynomial[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14448, 9)\n",
      "[4.13120000e+00 3.50000000e+01 1.70668134e+01 1.44592000e+02\n",
      " 1.22500000e+03 7.05064197e+01 5.97338470e+02 5.06072000e+03\n",
      " 4.28750000e+04]\n",
      "R2 for train: 0.5400481580698149\n",
      "R2 for test: 0.5358938035236791\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features_polynomial, y , test_size= 0.3, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_train[0])\n",
    "\n",
    "\n",
    "regression = LinearRegression()\n",
    "model = regression.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "r2_train = r2_score(y_train,y_pred_train)\n",
    "print(f\"R2 for train: {r2_train}\")\n",
    "\n",
    "r2_test = r2_score(y_test,y_pred_test)\n",
    "print(f\"R2 for test: {r2_test}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regularization is a technique that penalizes the **coefficient**.\n",
    "- In an overfit model, the **coefficients are generally inflated**. Thus, Regularization adds **penalties** to the parameters and avoids them weigh heavily.\n",
    "- The **coefficients** are added to the cost function of the linear equation. Thus, if the coefficient inflates, **the cost function will increase**. And Linear regression model will try to optimize the coefficient in order to minimize the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lasso Regularization (L1) :** \n",
    "\n",
    "- Also called **Least Absolute Shrinkage and Selection Operator**\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n(Y_i-\\sum_{j=1}^pX_{ij}\\beta_j)^2+\\lambda\\sum_{j=1}^p|\\beta_j|\n",
    "$$\n",
    "\n",
    "- You have many features, and you suspect that only a few of them are truly relevant to the response variable.\n",
    "- You want to perform feature selection, i.e., drive some coefficients exactly to zero, thus eliminating them from the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils._bunch.Bunch'>\n",
      "<class 'numpy.ndarray'>\n",
      "(20640, 9)\n",
      "R2 for train: 0.5461181785194866\n",
      "R2 for test: 0.545117728367666\n"
     ]
    }
   ],
   "source": [
    "# fetch data \n",
    "housing = fetch_california_housing()\n",
    "print(type(housing))\n",
    "print(type(housing.target))\n",
    "\n",
    "data = housing.data[:,:]\n",
    "target = housing.target\n",
    "\n",
    "# create dataframe \n",
    "df = pd.DataFrame(data, columns=[\"feature_0\",\"feature_1\",\"feature_2\",\n",
    "                                 \"feature_3\",\"feature_4\",\"feature_5\",\"feature_6\",\"feature_7\"])\n",
    "df[\"target\"] = target\n",
    "print(df.shape)\n",
    "\n",
    "X = df.loc[:,df.columns != \"target\"]\n",
    "y = df[\"target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "regression = Lasso(alpha=0.1)\n",
    "\n",
    "model = regression.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "r2_train = r2_score(y_train,y_pred_train)\n",
    "print(f\"R2 for train: {r2_train}\")\n",
    "\n",
    "r2_test = r2_score(y_test,y_pred_test)\n",
    "print(f\"R2 for test: {r2_test}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ridge regression(L2)**\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n(y_i-\\sum_{j=1}^px_{ij}\\beta_j)^2+\\lambda\\sum_{j=1}^p\\beta_j^2\n",
    "$$\n",
    "\n",
    "- if lambda is zero then you can imagine we get back OLS.\n",
    "\n",
    "When to use: \n",
    "\n",
    "- You have many features, and most of them are believed to have a small effect on the response variable.\n",
    "- Your goal is to handle multicollinearity by shrinking the coefficients of correlated variables.\n",
    "\n",
    "- Does not perform feature selection; all coefficients are shrunk towards zero but none are eliminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 for train: 0.6093459719471153\n",
      "R2 for test: 0.5957750170158791\n"
     ]
    }
   ],
   "source": [
    "regression = Ridge(alpha=0.1)\n",
    "\n",
    "model = regression.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "r2_train = r2_score(y_train,y_pred_train)\n",
    "print(f\"R2 for train: {r2_train}\")\n",
    "\n",
    "r2_test = r2_score(y_test,y_pred_test)\n",
    "print(f\"R2 for test: {r2_test}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Elastic net linear regression**\n",
    "- Elastic net linear regression uses the penalties from both the lasso and ridge techniques to regularize regression models.\n",
    "\n",
    "$$\n",
    "L_{enet}(\\hat{\\beta})=\\frac{\\sum_{i=1}^{n}(y_{i}-x_{i}^{\\prime}\\hat{\\beta})^{2}}{2n}+\\lambda(\\frac{1-\\alpha}{2}\\sum_{j=1}^{m}\\hat{\\beta}_{j}^{2}+\\alpha\\sum_{j=1}^{m}|\\hat{\\beta}_{j}|)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 for train: 0.5761912179593538\n",
      "R2 for test: 0.5755216146456115\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "regression = ElasticNet(alpha=0.1)\n",
    "\n",
    "model = regression.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "r2_train = r2_score(y_train,y_pred_train)\n",
    "print(f\"R2 for train: {r2_train}\")\n",
    "\n",
    "r2_test = r2_score(y_test,y_pred_test)\n",
    "print(f\"R2 for test: {r2_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
