{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Importing Libraries ##############\n",
    "import re\n",
    "import string\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "############ For NLP ###########################\n",
    "import spacy\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "########## Visualization #######################\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "############## For Machine Learning #############\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,ConfusionMatrixDisplay,confusion_matrix\n",
    "\n",
    "################ For Deep Learning ###############\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import BertModel\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Scrubbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset \n",
    "df = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/sugatagh/E-commerce-Text-Classification/main/Dataset/ecommerceDataset.csv', \n",
    "    names = ['label', 'description']\n",
    ")\n",
    "\n",
    "# switch columns \n",
    "df = df[['description', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping duplicate observations\n",
    "df.drop_duplicates(inplace = True)\n",
    "\n",
    "# shuffle the dataset as labels are not shuffled \n",
    "df = shuffle(df)\n",
    "\n",
    "# reset index after shuffeling \n",
    "df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\">Checking for Null Values</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "null = df.isnull().sum().to_dict()\n",
    "\n",
    "for k, v in null.items():\n",
    "    print(f\"Column : {k} has {v} null values.\" )\n",
    "    \n",
    "print(f\"Before dropping null, dataset has: {df.shape[0]} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the null values \n",
    "df = df.dropna(subset=['description'])\n",
    "print(f\"After dropping null, dataset has: {df.shape[0]} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\">Text Length Analysis</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the length of each text as new column\n",
    "df['text_length'] = df['description'].apply(len)\n",
    "\n",
    "# Get the unique categories\n",
    "categories = df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the histogram of length in each category.\n",
    "fig = go.Figure()\n",
    "for category in categories:\n",
    "    fig.add_trace(go.Histogram(x=df[df['label'] == category]['text_length'], \n",
    "                               name=category))\n",
    "\n",
    "# Update layout for better visualization\n",
    "fig.update_layout(barmode='stack',\n",
    "                  xaxis_title='Text Length',\n",
    "                  yaxis_title='Count',\n",
    "                  title='Histogram of Text Length in Each Category',\n",
    "                  autosize=False,\n",
    "                  width=1000,\n",
    "                  height=600)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like we have some descriptions with extreme lengths, we will consider these lengths as extreme outliers. Lets do box and whisker plot per category to get better idea of length distrubution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(df, x='label', y='text_length', color='label')\n",
    "\n",
    "# Update layout for better visualization\n",
    "fig.update_layout(title='Box Plot of Text Length for Each Category',\n",
    "                  autosize=False,\n",
    "                  width=1000,\n",
    "                  height=600)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will apply IQR technique to handle these extreme text lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to remove extreme length by IQR techinique based on text length.\n",
    "def remove_outliers_by_iqr(group):\n",
    "    Q1 = group['text_length'].quantile(0.25)\n",
    "    Q3 = group['text_length'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return group[(group['text_length'] >= lower_bound) & (group['text_length'] <= upper_bound)]\n",
    "\n",
    "# For each category, remove extreme length based on IQR\n",
    "df_filtered = df.groupby('label').apply(remove_outliers_by_iqr).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram for each category\n",
    "fig = go.Figure()\n",
    "for category in categories:\n",
    "    fig.add_trace(go.Histogram(x=df_filtered[df_filtered['label'] == category]['text_length'], \n",
    "                               name=category))\n",
    "\n",
    "# Update layout for better visualization\n",
    "fig.update_layout(barmode='stack',\n",
    "                  xaxis_title='Text Length',\n",
    "                  yaxis_title='Count',\n",
    "                  title='Histogram of Text Length in Each Category',\n",
    "                  autosize=False,\n",
    "                  width=1000,\n",
    "                  height=600)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\">Number of samples per category.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# some minimal colors\n",
    "colors = ['#494BD3', '#E28AE2', '#F1F481', '#79DB80', '#DF5F5F',\n",
    "            '#69DADE', '#C2E37D', '#E26580', '#D39F49', '#B96FE3']\n",
    "\n",
    "def cat_summary_with_graph(dataframe, col_name):\n",
    "\n",
    "    # Create subplots for bar abd pie chart.\n",
    "    fig = make_subplots(rows=1, cols=2,\n",
    "                        subplot_titles=('Records per class.', 'Percentage of classes.'),\n",
    "                        specs=[[{\"type\": \"xy\"}, {'type': 'domain'}]])\n",
    "    \n",
    "    # Code for count plot in plotly.\n",
    "    value_counts = dataframe[col_name].value_counts()\n",
    "    fig.add_trace(go.Bar(y=value_counts.values.tolist(),\n",
    "                         x=[str(i) for i in value_counts.index],\n",
    "                         text=value_counts.values.tolist(),\n",
    "                         textfont=dict(size=15),\n",
    "                         name=col_name,\n",
    "                         textposition='auto',\n",
    "                         showlegend=False,\n",
    "                         marker=dict(color=colors,\n",
    "                                     line=dict(color='#DBE6EC', width=1))),\n",
    "                  row=1, col=1)\n",
    "    \n",
    "    # Code for pie chart in plotly.\n",
    "    fig.add_trace(go.Pie(labels=value_counts.keys(),\n",
    "                         values=value_counts.values,\n",
    "                         textfont=dict(size=20),\n",
    "                         textposition='auto',\n",
    "                         showlegend=False,\n",
    "                         name=col_name,\n",
    "                         marker=dict(colors=colors)),\n",
    "                  row=1, col=2)\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(title={\n",
    "                             'y': 0.9,\n",
    "                             'x': 0.5,\n",
    "                             'xanchor': 'center',\n",
    "                             'yanchor': 'top'},\n",
    "                      template='plotly_white')\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# Call the function\n",
    "cat_summary_with_graph(df_filtered, 'label')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;\">NGrams Analysis.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Iterable\n",
    "\n",
    "class GetNgramsFrequency:\n",
    "    \"\"\"A class to generate and visualize n-gram frequency distributions from the given corpus.\n",
    "    \"\"\"\n",
    "    def __init__(self, text_list : Iterable) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the class with a list of text documents.\n",
    "\n",
    "        Args:\n",
    "            text_list (Iterable): A list of text documents.\n",
    "        \"\"\"\n",
    "        self.text_list = text_list\n",
    "        self.unigram = None\n",
    "        self.bigram = None\n",
    "        self.trigram = None\n",
    "        \n",
    "    def find_n_gram(self, size, top_n)->pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get the most frequent n-grams of a given size in the text documents.\n",
    "        Args:\n",
    "            size (int): The size of the n-grams.(e.g., 1 for unigrams, 2 for bigrams, 3 for trigrams).\n",
    "            top_n (int): The number of top n-grams.\n",
    "        Returns:\n",
    "            pd.DataFrame: A DataFrame with top n-grams and their counts.\n",
    "        \"\"\"\n",
    "        ngrams_all = []\n",
    "        for document in self.text_list:\n",
    "            tokens = document.split()\n",
    "            if len(tokens) <= size:\n",
    "                continue\n",
    "            else:\n",
    "                output = list(ngrams(tokens, size))\n",
    "            for ngram in output:\n",
    "                ngrams_all.append(\" \".join(ngram))\n",
    "        cnt_ngram = Counter()\n",
    "        for word in ngrams_all:\n",
    "            cnt_ngram[word] += 1\n",
    "        df = pd.DataFrame.from_dict(cnt_ngram, orient='index').reset_index()\n",
    "        df = df.rename(columns={'index':'words', 0:'count'})\n",
    "        df = df.sort_values(by='count', ascending=False)\n",
    "        df = df.head(top_n)\n",
    "        df = df.sort_values(by='count')\n",
    "        return(df)\n",
    "    \n",
    "\n",
    "    def generate_n_grams(self, top_n)-> None:\n",
    "        \"\"\"\n",
    "        Find and store the most frequent unigrams, bigrams, and trigrams.\n",
    "        Args:\n",
    "            top_n (int): The number of top n-grams for each n-gram size.\n",
    "        \"\"\"\n",
    "        self.unigram = self.find_n_gram(size=1, top_n=top_n)\n",
    "        self.bigram = self.find_n_gram(size=2, top_n=top_n)\n",
    "        self.trigram = self.find_n_gram(size=3, top_n=top_n)\n",
    "        \n",
    "        \n",
    "    def plot_distribution(self)->None:\n",
    "        \"\"\"Plots the frequency distribution of unigrams, bigrams, and trigrams.\n",
    "        \"\"\"\n",
    "\n",
    "        fig = make_subplots(rows=1, cols=3, subplot_titles=('Unigrams', 'Bigrams', 'Trigrams'))\n",
    "        fig.add_trace(go.Bar(x=self.unigram['count'], y=self.unigram['words'], orientation='h', marker=dict(opacity=0.5)), row=1, col=1)\n",
    "        fig.add_trace(go.Bar(x=self.bigram['count'], y=self.bigram['words'], orientation='h', marker=dict(opacity=0.5)), row=1, col=2)\n",
    "        fig.add_trace(go.Bar(x=self.trigram['count'], y=self.trigram['words'], orientation='h', marker=dict(opacity=0.5)), row=1, col=3)\n",
    "        fig.update_layout(height=700, width=1500, showlegend=False)\n",
    "        fig.update_xaxes(title_text='Count', row=1, col=1)\n",
    "        fig.update_xaxes(title_text='Count', row=1, col=2)\n",
    "        fig.update_xaxes(title_text='Count', row=1, col=3)\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the GetNgramsFrequency object \n",
    "n_gram_analyzer = GetNgramsFrequency(text_list=df_filtered[\"description\"])\n",
    "\n",
    "# find top n n-grams\n",
    "n_gram_analyzer.generate_n_grams(top_n=10)\n",
    "\n",
    "# plot the frequncy distribution\n",
    "n_gram_analyzer.plot_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our custom tokenizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"tokenize text into words using a regular expression-based tokenizer\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        self.tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "\n",
    "    def word_tokenizer(self, text) -> Iterable:\n",
    "        \"\"\"Tokenizes the input text into words\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to be tokenized.\n",
    "\n",
    "        Returns:\n",
    "            Iterable: A list of tokens (words).\n",
    "        \"\"\"\n",
    "        return self.tokenizer.tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCleaner(Tokenizer):\n",
    "    \"\"\"A class for cleaning text data, extending the Tokenizer\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Initialize the TextCleaner class by loading the acronyms and contractions dictionaries.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # the acronyms url\n",
    "        self._acronyms_url = \"https://raw.githubusercontent.com/sugatagh/E-commerce-Text-Classification/main/JSON/english_acronyms.json\"\n",
    "\n",
    "        # link to data where contractios list is present\n",
    "        self._contractions_url = \"https://raw.githubusercontent.com/sugatagh/E-commerce-Text-Classification/main/JSON/english_contractions.json\"\n",
    "\n",
    "        # load the acronym dict\n",
    "        self._acronyms_dict = self.load_acronym()\n",
    "        # load acronym list\n",
    "        self._acronym_list = list(self._acronyms_dict.keys())\n",
    "\n",
    "        # load the contractions dict\n",
    "        self._contractions_dict = self.load_contractions()\n",
    "        # load contractions list\n",
    "        self._contractions_list = list(self._contractions_dict.keys())\n",
    "\n",
    "    def load_acronym(self) -> pd.Series:\n",
    "        \"\"\"Loads the acronyms dictionary from the specified URL.\n",
    "        Returns:\n",
    "             pd.Series: Retrun dictionary as pandas series.\n",
    "        \"\"\"\n",
    "        return pd.read_json(self._acronyms_url, typ=\"series\")\n",
    "\n",
    "    def load_contractions(self):\n",
    "        \"\"\"Loads the contractions dictionary from the specified URL.\n",
    "        Returns:\n",
    "             pd.Series: Retrun dictionary as pandas series.\n",
    "        \"\"\"\n",
    "        return pd.read_json(self._contractions_url, typ=\"series\")\n",
    "\n",
    "    # Converting to lowercase\n",
    "    def convert_to_lowercase(self, text):\n",
    "        return text.lower()\n",
    "\n",
    "    # remove whitespace from the text\n",
    "    def remove_whitespace(self, text):\n",
    "        return text.strip()\n",
    "\n",
    "    # Removing punctuations from the given string\n",
    "    def remove_punctuation(self, text):\n",
    "        # get all the punctuations\n",
    "        punct_str = string.punctuation\n",
    "\n",
    "        # the apostrophe will be remove using contraction.\n",
    "        punct_str = punct_str.replace(\"'\", \"\")\n",
    "        return text.translate(str.maketrans(\"\", \"\", punct_str))\n",
    "\n",
    "    # Remove any HTML if present in the text.\n",
    "    def remove_html(self, text):\n",
    "        html = re.compile(r\"<.*?>\")\n",
    "        return html.sub(r\"\", text)\n",
    "\n",
    "    # Remove URLs\n",
    "    def remove_http(self, text):\n",
    "        http = \"https?://\\S+|www\\.\\S+\"  # matching strings beginning with http (but not just \"http\")\n",
    "        pattern = r\"({})\".format(http)  # creating pattern\n",
    "        return re.sub(pattern, \"\", text)\n",
    "\n",
    "    # Remove any Emojis present in the text.\n",
    "    def remove_emoji(self, text):\n",
    "        emoji_pattern = re.compile(\n",
    "            \"[\"\n",
    "            \"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "            \"\\U00002702-\\U000027B0\"\n",
    "            \"\\U000024C2-\\U0001F251\"\n",
    "            \"]+\",\n",
    "            flags=re.UNICODE,\n",
    "        )\n",
    "        return emoji_pattern.sub(r\"\", text)\n",
    "\n",
    "    def convert_acronyms(self, text):\n",
    "        words = []\n",
    "        for word in self.word_tokenizer(text):\n",
    "            if word in self._acronym_list:\n",
    "                words = words + self._acronyms_dict[word].split()\n",
    "            else:\n",
    "                words = words + word.split()\n",
    "\n",
    "        text_converted = \" \".join(words)\n",
    "        return text_converted\n",
    "\n",
    "    def convert_contractions(self, text):\n",
    "        words = []\n",
    "        for word in self.word_tokenizer(text):\n",
    "            if word in self._contractions_list:\n",
    "                words = words + self._contractions_dict[word].split()\n",
    "            else:\n",
    "                words = words + word.split()\n",
    "\n",
    "        text_converted = \" \".join(words)\n",
    "        return text_converted\n",
    "\n",
    "    def __call__(self, text):\n",
    "        \"\"\"\n",
    "        Cleans the input text.\n",
    "        Applies a series of preprocessing steps, including:\n",
    "        converting to lowercase, removing whitespace, punctuation, HTML tags, URLs, and emojis,\n",
    "        and converting acronyms and contractions to their expanded forms.\n",
    "        Args:\n",
    "            text (str): The text to be cleaned.\n",
    "\n",
    "        Returns:\n",
    "            str: The cleaned text.\n",
    "        \"\"\"\n",
    "        text = self.convert_to_lowercase(text=text)\n",
    "        text = self.remove_whitespace(text=text)\n",
    "        text = self.remove_punctuation(text=text)\n",
    "        text = self.remove_html(text=text)\n",
    "        text = self.remove_http(text=text)\n",
    "        text = self.remove_emoji(text=text)\n",
    "        text = self.convert_acronyms(text=text)\n",
    "        text = self.convert_contractions(text=text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the text cleaner \n",
    "text_cleaner = TextCleaner()\n",
    "\n",
    "\n",
    "# apply the text cleaner to the descriptions \n",
    "df_filtered[\"cleaned_description\"] = df_filtered[\"description\"].apply(text_cleaner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Processing and Normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepositions = [\n",
    "    \"about\",\n",
    "    \"above\",\n",
    "    \"across\",\n",
    "    \"after\",\n",
    "    \"against\",\n",
    "    \"among\",\n",
    "    \"around\",\n",
    "    \"at\",\n",
    "    \"before\",\n",
    "    \"behind\",\n",
    "    \"below\",\n",
    "    \"beside\",\n",
    "    \"between\",\n",
    "    \"by\",\n",
    "    \"down\",\n",
    "    \"during\",\n",
    "    \"for\",\n",
    "    \"from\",\n",
    "    \"in\",\n",
    "    \"inside\",\n",
    "    \"into\",\n",
    "    \"near\",\n",
    "    \"of\",\n",
    "    \"off\",\n",
    "    \"on\",\n",
    "    \"out\",\n",
    "    \"over\",\n",
    "    \"through\",\n",
    "    \"to\",\n",
    "    \"toward\",\n",
    "    \"under\",\n",
    "    \"up\",\n",
    "    \"with\",\n",
    "]\n",
    "prepositions_less_common = [\n",
    "    \"aboard\",\n",
    "    \"along\",\n",
    "    \"amid\",\n",
    "    \"as\",\n",
    "    \"beneath\",\n",
    "    \"beyond\",\n",
    "    \"but\",\n",
    "    \"concerning\",\n",
    "    \"considering\",\n",
    "    \"despite\",\n",
    "    \"except\",\n",
    "    \"following\",\n",
    "    \"like\",\n",
    "    \"minus\",\n",
    "    \"onto\",\n",
    "    \"outside\",\n",
    "    \"per\",\n",
    "    \"plus\",\n",
    "    \"regarding\",\n",
    "    \"round\",\n",
    "    \"since\",\n",
    "    \"than\",\n",
    "    \"till\",\n",
    "    \"underneath\",\n",
    "    \"unlike\",\n",
    "    \"until\",\n",
    "    \"upon\",\n",
    "    \"versus\",\n",
    "    \"via\",\n",
    "    \"within\",\n",
    "    \"without\",\n",
    "]\n",
    "coordinating_conjunctions = [\"and\", \"but\", \"for\", \"nor\", \"or\", \"so\", \"and\", \"yet\"]\n",
    "correlative_conjunctions = [\n",
    "    \"both\",\n",
    "    \"and\",\n",
    "    \"either\",\n",
    "    \"or\",\n",
    "    \"neither\",\n",
    "    \"nor\",\n",
    "    \"not\",\n",
    "    \"only\",\n",
    "    \"but\",\n",
    "    \"whether\",\n",
    "    \"or\",\n",
    "]\n",
    "subordinating_conjunctions = [\n",
    "    \"after\",\n",
    "    \"although\",\n",
    "    \"as\",\n",
    "    \"as if\",\n",
    "    \"as long as\",\n",
    "    \"as much as\",\n",
    "    \"as soon as\",\n",
    "    \"as though\",\n",
    "    \"because\",\n",
    "    \"before\",\n",
    "    \"by the time\",\n",
    "    \"even if\",\n",
    "    \"even though\",\n",
    "    \"if\",\n",
    "    \"in order that\",\n",
    "    \"in case\",\n",
    "    \"in the event that\",\n",
    "    \"lest\",\n",
    "    \"now that\",\n",
    "    \"once\",\n",
    "    \"only\",\n",
    "    \"only if\",\n",
    "    \"provided that\",\n",
    "    \"since\",\n",
    "    \"so\",\n",
    "    \"supposing\",\n",
    "    \"that\",\n",
    "    \"than\",\n",
    "    \"though\",\n",
    "    \"till\",\n",
    "    \"unless\",\n",
    "    \"until\",\n",
    "    \"when\",\n",
    "    \"whenever\",\n",
    "    \"where\",\n",
    "    \"whereas\",\n",
    "    \"wherever\",\n",
    "    \"whether or not\",\n",
    "    \"while\",\n",
    "]\n",
    "\n",
    "\n",
    "class TextPreprocess(Tokenizer):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # initialize and update the stop words\n",
    "        self.stopwords = stopwords.words(\"english\") + [\n",
    "            \"among\",\n",
    "            \"onto\",\n",
    "            \"shall\",\n",
    "            \"thrice\",\n",
    "            \"thus\",\n",
    "            \"twice\",\n",
    "            \"unto\",\n",
    "            \"us\",\n",
    "            \"would\",\n",
    "        ]\n",
    "\n",
    "        # spell checker\n",
    "        self.spell = SpellChecker()\n",
    "\n",
    "        # stemmer object\n",
    "        self.stemmer = PorterStemmer()\n",
    "\n",
    "        # spacy lemmatizer object\n",
    "        self.spacy_lemmatizer = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "        # remove additional stop words\n",
    "        self.additional_stop_words = (\n",
    "            prepositions\n",
    "            + prepositions_less_common\n",
    "            + coordinating_conjunctions\n",
    "            + correlative_conjunctions\n",
    "        )\n",
    "\n",
    "    def remove_stopwords(self, word_list):\n",
    "        return [word for word in word_list if word not in self.stopwords]\n",
    "\n",
    "    def pyspellchecker(self, word_list):\n",
    "        unknown_words = self.spell.unknown(word_list)\n",
    "        return [self.spell.correction(word) if word in unknown_words else word for word in word_list]\n",
    "\n",
    "    def porter_stemmer(self, word_list):\n",
    "        return [self.stemmer.stem(word) for word in word_list if word is not None]\n",
    "\n",
    "\n",
    "    def lemmatizer(self, word_list):\n",
    "        return [token.lemma_ for token in self.spacy_lemmatizer(\" \".join(word_list))]\n",
    "\n",
    "    def remove_additional_stopwords(self, word_list):\n",
    "        return [word for word in word_list if word not in self.additional_stop_words]\n",
    "\n",
    "    def __call__(self, text):\n",
    "        try:\n",
    "            text = text.strip()\n",
    "            word_list = self.word_tokenizer(text)\n",
    "            word_list = self.remove_stopwords(word_list)\n",
    "            # word_list = self.pyspellchecker(word_list)\n",
    "            word_list = self.porter_stemmer(word_list)\n",
    "            word_list = self.lemmatizer(word_list)\n",
    "            word_list = self.remove_additional_stopwords(word_list)\n",
    "\n",
    "            return \" \".join(word_list)\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Initialize the text processor\n",
    "text_processor = TextPreprocess()\n",
    "\n",
    "cleaned = []\n",
    "\n",
    "for text in tqdm(df_filtered[\"cleaned_description\"]):\n",
    "    cleaned.append(text_processor(text=text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the final text in the dataframe\n",
    "df_filtered[\"text\"] = cleaned\n",
    "\n",
    "# remove all the intermediate results to save memory \n",
    "del df_filtered[\"cleaned_description\"]\n",
    "\n",
    "\n",
    "del df_filtered[\"description\"]\n",
    "\n",
    "\n",
    "del df_filtered[\"text_length\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize bow vectorizer where whe want 1-2 n-gram which are repeated\n",
    "# atleast twice where max_df=0.9 means words occurring in more than 90% of the documents will be ignored\n",
    "\n",
    "bow_vectorizer = CountVectorizer(max_df=0.9,min_df=2,ngram_range=(1,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fit the bag of words on given text\n",
    "X_bow = bow_vectorizer.fit_transform(df_filtered['text'])\n",
    "\n",
    "# Get the vocabulary (word to index mapping)\n",
    "vocab = bow_vectorizer.vocabulary_\n",
    "\n",
    "# Preparation for the word cloud\n",
    "reverse_vocab = {idx: word for word, idx in vocab.items()}\n",
    "word_counts = X_bow.sum(axis=0)\n",
    "word_freq = {reverse_vocab[idx]: word_counts[0, idx] for idx in range(len(vocab))}\n",
    "wordcloud = WordCloud(width=1000, height=500, background_color='white').generate_from_frequencies(word_freq)\n",
    "# Show the word cloud\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud - Bag of Words (BoW)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize tf-idf\n",
    "# vectorizer where whe want 1-2 n-gram which are repeated\n",
    "# atleast twice where max_df=0.9 means words occurring in more than 90% of the documents will be ignored\n",
    "\n",
    "tf_idf = TfidfVectorizer(min_df=2,max_df=0.90,ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the bag of words on given text\n",
    "X_tfidf = tf_idf.fit_transform(df_filtered['text'])\n",
    "\n",
    "# Get the vocabulary (word to index mapping)\n",
    "vocab = tf_idf.vocabulary_\n",
    "\n",
    "# Preparation for the word cloud\n",
    "reverse_vocab = {idx: word for word, idx in vocab.items()}\n",
    "word_counts = X_tfidf.sum(axis=0)\n",
    "word_freq = {reverse_vocab[idx]: word_counts[0, idx] for idx in range(len(vocab))}\n",
    "wordcloud = WordCloud(width=1000, height=500, background_color='white').generate_from_frequencies(word_freq)\n",
    "# Show the word cloud\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud - TF-IDF')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding Lables of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the label encoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# apply on the data\n",
    "df_filtered['label'] = le.fit_transform(df_filtered['label'])\n",
    "\n",
    "# reshuffle\n",
    "df_filtered = shuffle(df_filtered)\n",
    "\n",
    "# reset index\n",
    "df_filtered.reset_index(drop = True, inplace = True)\n",
    "\n",
    "print(f\"Final data for training: \")\n",
    "df_filtered.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data for train test : \n",
    "X = df_filtered['text'] \n",
    "y = df_filtered['label']\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bag of Words + MultinomialNB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline with bow+multinominalnb\n",
    "pipe_bow_nb = Pipeline([('vectorizer', bow_vectorizer),\n",
    "                    ('classifier', MultinomialNB())])\n",
    "\n",
    "# fit on the data\n",
    "pipe_bow_nb.fit(X_train,y_train)\n",
    "\n",
    "# get the predictions\n",
    "y_pred_bow_nb = pipe_bow_nb.predict(X_test)\n",
    "\n",
    "# print the classification report \n",
    "\n",
    "print(classification_report(y_test,y_pred_bow_nb))\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_test,y_pred_bow_nb),display_labels=['Books','Clothing','Electronics','Household']).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bag of Words + RandomForestClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline with bow+RandomForestClassifier\n",
    "pipe_bow_rf = Pipeline([('vectorizer', bow_vectorizer),\n",
    "                    ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# fit on the data\n",
    "pipe_bow_rf.fit(X_train,y_train)\n",
    "\n",
    "# get the predictions\n",
    "y_pred_bow_rf = pipe_bow_rf.predict(X_test)\n",
    "\n",
    "# print the classification report \n",
    "\n",
    "print(classification_report(y_test,y_pred_bow_rf))\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_test,y_pred_bow_rf),display_labels=['Books','Clothing','Electronics','Household']).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF + MultinomialNB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline with Tfidf+multinominalnb\n",
    "pipe_tfidf_nb = Pipeline([('vectorizer', tf_idf),\n",
    "                    ('classifier', MultinomialNB())])\n",
    "\n",
    "# fit on the data\n",
    "pipe_tfidf_nb.fit(X_train,y_train)\n",
    "\n",
    "# get the predictions\n",
    "y_pred_tfidf_nb = pipe_tfidf_nb.predict(X_test)\n",
    "\n",
    "# print the classification report \n",
    "\n",
    "print(classification_report(y_test,y_pred_tfidf_nb))\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_test,y_pred_tfidf_nb),display_labels=['Books','Clothing','Electronics','Household']).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TFIDF + RandomForestClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pipeline with bow+RF\n",
    "pipe_rf_knn = Pipeline([('vectorizer', tf_idf),\n",
    "                    ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# fit on the data\n",
    "pipe_rf_knn.fit(X_train,y_train)\n",
    "\n",
    "# get the predictions\n",
    "y_pred_bow_rf = pipe_rf_knn.predict(X_test)\n",
    "\n",
    "# print the classification report \n",
    "\n",
    "print(classification_report(y_test,y_pred_bow_rf))\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_test,y_pred_bow_rf),display_labels=['Books','Clothing','Electronics','Household']).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorDataLoader(Dataset):\n",
    "    def __init__(self, input_ids, attention_mask, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareDataset:\n",
    "    \n",
    "    def __init__(self, dataframe, text_column_name : str, labels_column_name: str, encoding : bool = False, split : float = 0.75) -> None:\n",
    "        self.df = dataframe\n",
    "        \n",
    "        self.text_column_name = text_column_name\n",
    "        self.labels_column_name = labels_column_name\n",
    "        \n",
    "        if encoding:\n",
    "            self.convert_label()\n",
    "            \n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "        \n",
    "        self.split = split\n",
    "        \n",
    "        self.unique_labels = self.df[self.labels_column_name].nunique()\n",
    "    \n",
    "    def convert_label(self):\n",
    "        unique = self.df[self.labels_column_name].unique()\n",
    "        map_value = {v: i for i, v in enumerate(unique)}\n",
    "        self.df[f\"{self.labels_column_name}\"] = self.df[self.labels_column_name].map(map_value)\n",
    "\n",
    "\n",
    "    def get_input_id_attention_mask(self, sentence):\n",
    "        tokens = self.tokenizer.encode_plus(\n",
    "        sentence,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_token_type_ids=False,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "        return tokens['input_ids'].squeeze(), tokens['attention_mask'].squeeze()\n",
    "    \n",
    "    \n",
    "    def get_data_loader(self):\n",
    "        \n",
    "        ids = torch.zeros(((len(df), 512)), dtype=torch.long)\n",
    "        masks = torch.zeros(((len(df), 512)), dtype=torch.long)\n",
    "\n",
    "        # Iterate over each text in the 'text' column of the DataFrame\n",
    "        for i, txt in enumerate(self.df[self.text_column_name]):\n",
    "            # Tokenize the current text using the tokenize function\n",
    "            input_ids, attention_masks = self.get_input_id_attention_mask(txt)\n",
    "            \n",
    "            # Assign the tokenized input IDs to the corresponding row in the ids tensor\n",
    "            ids[i,:]=input_ids\n",
    "            \n",
    "            # Assign the attention masks to the corresponding row in the masks tensor\n",
    "            masks[i,:]=attention_masks\n",
    "            \n",
    "        arr = df[self.labels_column_name].values\n",
    "        print(arr[0], type(arr[0]))\n",
    "        labels = torch.zeros((arr.size, arr.max()+1), dtype=torch.float)\n",
    "        labels[torch.arange(arr.size), arr] = 1\n",
    "        \n",
    "        dataset = TensorDataLoader(ids, masks, labels)\n",
    "        \n",
    "        train_size = int(len(dataset) * self.split)\n",
    "        test_size = len(dataset) - train_size\n",
    "\n",
    "        train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "        \n",
    "        return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_data_loader = PrepareDataset(dataframe=df_filtered,\n",
    "                                  text_column_name=\"text\",\n",
    "                                  labels_column_name=\"label\",\n",
    "                                  encoding=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the data loader, prepare dataset using PrepareDataset class\n",
    "train_dataset, test_dataset = bert_data_loader.get_data_loader()\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "# Define NN Architecture\n",
    "class ClassificationModel(nn.Module):\n",
    "    def __init__(self, bert, n_lables : int):\n",
    "        \"\"\"\n",
    "        Initialize the sentiment classifier model with BERT.\n",
    "        \n",
    "        Parameters:\n",
    "        - bert (BertModel): Pretrained BERT model.\n",
    "        \"\"\"\n",
    "        super(ClassificationModel, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dropout = nn.Dropout(p=0.25)\n",
    "        self.fc = nn.Linear(bert.config.hidden_size, n_lables)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the sentiment classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        - input_ids (torch.Tensor): Input IDs tensor.\n",
    "        - attention_mask (torch.Tensor): Attention mask tensor.\n",
    "        \n",
    "        Returns:\n",
    "        - torch.Tensor: Output logits.\n",
    "        \"\"\"\n",
    "        # Pass the inputs through the BERT model\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Use the pooled output from BERT\n",
    "        pooled_output = outputs.pooler_output\n",
    "        \n",
    "        # Apply dropout\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        # Pass through the fully connected layer to get logits\n",
    "        logits = self.fc(pooled_output)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Bert Architecture\n",
    "bert = BertModel.from_pretrained('bert-base-cased')\n",
    "\n",
    "# Initialize NN Architecture\n",
    "BertClassifier = ClassificationModel(bert,bert_data_loader.unique_labels)\n",
    "\n",
    "# set the unfreeze layer 10 or layer 11\n",
    "for name, param in BertClassifier.bert.named_parameters():\n",
    "    if 'layer.11' in name or 'layer.10' in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "        \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)\n",
    "\n",
    "# if multiple devices, convert the model into model parallel.\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    BertClassifier = nn.DataParallel(BertClassifier)\n",
    "    \n",
    "else:\n",
    "    # if not, just convert the model into device\n",
    "    BertClassifier.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No of Epochs\n",
    "epochs = 15\n",
    "\n",
    "# Define criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, BertClassifier.parameters()), lr=2e-5)\n",
    "\n",
    "# Define Scheduler with total seteps \n",
    "total_steps = len(train_loader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, test_loss = [], []\n",
    "train_acc, test_acc = [], []\n",
    "\n",
    "all_epoch_preds, all_epoch_labels = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Set the model to training mode\n",
    "    BertClassifier.train()\n",
    "    total_loss, total_correct = 0, 0\n",
    "    total_samples = 0  # Initialize total samples for train\n",
    "\n",
    "    # Create a progress bar for the training loop\n",
    "    train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\")\n",
    "\n",
    "    # Iterate over the training batches\n",
    "    for i, batch in enumerate(train_loader_tqdm):\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move the batch data to the device\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # Convert one-hot encoded labels to class indices\n",
    "        labels = torch.argmax(labels, dim=1)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        outputs = BertClassifier(input_ids, attention_mask)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backpropagate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the model parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the total loss, correct predictions, and total samples\n",
    "        total_loss += loss.item()\n",
    "        total_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "        total_samples += len(batch[\"input_ids\"])\n",
    "\n",
    "        # Update the progress bar with the current loss and accuracy\n",
    "        train_loader_tqdm.set_postfix({\"loss\": f\"{loss.item():.4f}\",\n",
    "                                        \"acc\": f\"{(outputs.argmax(dim=1) == labels).float().mean().item() * 100:.2f}%\"})\n",
    "\n",
    "    # Update the learning rate scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # Calculate the average training loss and accuracy\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_acc = total_correct / total_samples\n",
    "\n",
    "    # Append the training loss and accuracy to the lists\n",
    "    train_loss.append(avg_loss)\n",
    "    train_acc.append(avg_acc)\n",
    "\n",
    "    # Print the training loss and accuracy for the current epoch\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {avg_loss:.4f}, Training Accuracy: {avg_acc * 100:.2f}%\")\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    BertClassifier.eval()\n",
    "    total_test_loss, total_test_correct = 0, 0\n",
    "    total_test_samples = 0  # Initialize total samples for test\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Create a progress bar for the test loop\n",
    "    test_loader_tqdm = tqdm(test_loader, desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\")\n",
    "\n",
    "    # Disable gradient calculation for the test loop\n",
    "    with torch.no_grad():\n",
    "        # Iterate over the test batches\n",
    "        for i, batch in enumerate(test_loader_tqdm):\n",
    "            # Move the batch data to the device\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # Convert one-hot encoded labels to class indices\n",
    "            labels = torch.argmax(labels, dim=1)\n",
    "\n",
    "            # Forward pass through the model\n",
    "            outputs = BertClassifier(input_ids, attention_mask)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Update the total validation loss, correct predictions, and total samples\n",
    "            total_test_loss += loss.item()\n",
    "            total_test_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "            total_test_samples += len(batch[\"input_ids\"])\n",
    "\n",
    "            # Collect predictions and true labels\n",
    "            all_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            # Update the progress bar with the current validation loss and accuracy\n",
    "            test_loader_tqdm.set_postfix({\"loss\": f\"{loss.item():.4f}\",\n",
    "                                           \"acc\": f\"{(outputs.argmax(dim=1) == labels).float().mean().item() * 100:.2f}%\"})\n",
    "\n",
    "    # Calculate the average validation loss and accuracy\n",
    "    avg_test_loss = total_test_loss / len(test_loader)\n",
    "    avg_test_acc = total_test_correct / total_test_samples\n",
    "\n",
    "    # Append the validation loss and accuracy to the lists\n",
    "    test_loss.append(avg_test_loss)\n",
    "    test_acc.append(avg_test_acc)\n",
    "\n",
    "    # Append epoch predictions and labels to the lists\n",
    "    all_epoch_preds.extend(all_preds)\n",
    "    all_epoch_labels.extend(all_labels)\n",
    "\n",
    "    # Print the validation loss and accuracy for the current epoch\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Test Loss: {avg_test_loss:.4f}, Test Accuracy: {avg_test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "ax1.plot(train_loss, label='Train Loss', color='blue')\n",
    "ax1.plot(test_loss, label='Test Loss', color='red')\n",
    "ax1.set_title('Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "\n",
    "# Plotting train and test accuracy on the second subplot\n",
    "ax2.plot(train_acc, label='Train Accuracy', color='green')\n",
    "ax2.plot(test_acc, label='Test Accuracy', color='orange')\n",
    "ax2.set_title('Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for all epoch predictions and true labels\n",
    "\n",
    "df_results = pd.DataFrame({\"Predictions\": all_epoch_preds, \"True Labels\": all_epoch_labels})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = df_results['Predictions']\n",
    "y_true = df_results['True Labels']\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y_true, y_pred)\n",
    "\n",
    "print(report)\n",
    "\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_true, y_pred),display_labels=['Books','Clothing','Electronics','Household']).plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
