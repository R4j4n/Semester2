{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Cheat Sheet</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Anchors**\n",
    "- `\\A` : Matches only at the beginning of the string\n",
    "- `\\Z` : Matches only at the end of the string.\n",
    "- `^` :  Matches at the beginning of a line.\n",
    "- `$` : Matches at the end of a line.\n",
    "- `\\n` : Matches a newline character.\n",
    "- `re.MULTILINE` or `re.M` : Makes ^ and $ match the start and end of each line (not just the start and end of the string). \n",
    "- `\\b` : Matches the boundary between a word and a non-word character.\n",
    "- `\\B` :  Matches positions where `\\b` does not match (inside words, or between non-word characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 5), match='Hello'>\n",
      "None\n",
      "None\n",
      "<re.Match object; span=(6, 12), match='World!'>\n",
      "<re.Match object; span=(6, 12), match='World!'>\n",
      "<re.Match object; span=(0, 5), match='Hello'>\n",
      "None\n",
      "<re.Match object; span=(6, 11), match='World'>\n",
      "<re.Match object; span=(6, 12), match='World!'>\n",
      "None\n",
      "<re.Match object; span=(0, 5), match='Hello'>\n",
      "<re.Match object; span=(0, 5), match='Hello'>\n",
      "None\n",
      "<re.Match object; span=(1, 5), match='word'>\n",
      "None\n",
      "<re.Match object; span=(2, 4), match='ll'>\n"
     ]
    }
   ],
   "source": [
    "# Examples \n",
    "\n",
    "# \\A\n",
    "print(re.search(\"\\AHello\", string=\"Hello World\"))\n",
    "\n",
    "# Doesnot consider the line break, begining or not. \n",
    "print(re.search(\"\\AWorld\", string=\"Hello\\nWorld\")) # Not Found\n",
    "print(re.search(\"\\AWorld\", string=\"Hello\\nWorld\", flags=re.MULTILINE)) # Still not Found\n",
    "\n",
    "\n",
    "# \\Z\n",
    "print(re.search(r'World!\\Z', 'Hello World!'))\n",
    "\n",
    "print(re.search(r'World!\\Z', 'Hello\\nWorld!')) # Match Found\n",
    "\n",
    "\n",
    "# ^\n",
    "print(re.search(\"^Hello\", string=\"Hello World\"))\n",
    "print(re.search(\"^World\", string=\"Hello\\nWorld\")) # No Match\n",
    "print(re.search(\"^World\", string=\"Hello\\nWorld\", flags=re.MULTILINE)) # Found\n",
    "\n",
    "\n",
    "# $ \n",
    "print(re.search(r'World!$', 'Hello\\nWorld!'))\n",
    "print(re.search(r'Hello$', 'Hello\\nWorld!')) # No Match\n",
    "print(re.search(r'Hello$', 'Hello\\nWorld!', re.MULTILINE)) # Found\n",
    "\n",
    "\n",
    "# \\b\n",
    "print(re.search(r'\\bHello\\b', 'Hello World!')) # Match Found\n",
    "print(re.search(r'\\bHello\\b', 'HelloWorld!')) # No Match\n",
    "\n",
    "# \\B\n",
    "# Matches \"word\" only if it is not at a word boundary\n",
    "print(re.search(r'\\Bword\\B', 'swordfish'))            # Match found\n",
    "print(re.search(r'\\Bword\\B', 'a word in a sentence')) # No match\n",
    "print(re.search(r'\\Bll\\B', 'Hello'))                  # Match found\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Features**\n",
    "\n",
    "- `|` : Conditional OR, Combines multiple regular expressions as alternatives.Each alternative can have independent anchors.\n",
    "- `(pat)` : Capturing Group, Groups a pattern or patterns. Also captures the matched substring for back-references.\n",
    "- `(?:pat)` : Non-Capturing Group, Groups a pattern or patterns without capturing the matched substring. \n",
    "- `?P<name>pat` :  Named Capture Group, Groups a pattern and assigns a name to the captured substring. \n",
    "- `.` :  Matches any single character except the newline character.\n",
    "- `[]`: Matches one character among many specified inside the brackets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(9, 12), match='cat'>\n",
      "None\n",
      "Hello\n",
      "World\n",
      "Hello\n",
      "World\n",
      "Hello World\n",
      "('World',)\n",
      "None\n",
      "<re.Match object; span=(0, 5), match='Hillo'>\n",
      "<re.Match object; span=(1, 2), match='e'>\n",
      "None\n",
      "<re.Match object; span=(3, 4), match='a'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# | \n",
    "print(re.search(r'cat|dog', 'I have a cat'))\n",
    "print(re.search(r'cat|dog', 'I have a fish') ) # No Match\n",
    "\n",
    "# (pat)\n",
    "match = re.search(r'(Hello) (World)', 'Hello World')\n",
    "print(match.group(1))  # Outputs: Hello\n",
    "print(match.group(2))  # Outputs: World\n",
    "\n",
    "text = \"Hello\\nWorld\"\n",
    "match = re.search(r\"(^H\\w+)\\n(^W\\w+)\", text, re.MULTILINE)\n",
    "print(match.group(1))  # Outputs: Hello\n",
    "print(match.group(2))  # Outputs: World\n",
    "\n",
    "\n",
    "# (?:pat)\n",
    "# capture chai gar but use chai na gar. \n",
    "#  is a non-capturing group used to group parts of the pattern together without capturing them as separate groups.\n",
    "match = re.search(r'(?:Hello) (World)', 'Hello World')\n",
    "print(match.group(0))  # Outputs: Hello World\n",
    "print(match.groups())  # Outputs: ('World',)\n",
    "\n",
    "\n",
    "# . \n",
    "print(re.search(r'H.llo', 'H\\nllo')) # No match\n",
    "print(re.search(r'H.llo', 'Hillo'))  # Match found\n",
    "\n",
    "# []\n",
    "print(re.search(r'[aeiou]', 'Hello'))  # Match found\n",
    "print(re.search(r'[aeiou]', 'Sky'))    # No match\n",
    "print(re.search(r'[a-z]', '123abc'))   # Match found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Greedy Quantifiers**\n",
    "\n",
    "- `*` : Matches the preceding element zero or more times.\n",
    "- `+` : Matches the preceding element one or more times.\n",
    "- `?` : Matches the preceding element zero or one time.\n",
    "- `{m,n}` : Matches the preceding element at least m times, but not more than n times.\n",
    "- `{m,}` : Matches the preceding element at least m times.\n",
    "- `{,n}` : Matches the preceding element at most n times.\n",
    "- `{n}` : Matches the preceding element exactly n times.\n",
    "- `pat1.*pat2`:  Matches pat1 followed by any number of any characters (including none), and then pat2.\n",
    "- `pat1.*pat2|pat2.*pat1` : Matches pat1 followed by any characters and then pat2, or pat2 followed by any characters and then pat1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='a'>\n",
      "<re.Match object; span=(0, 2), match='ab'>\n",
      "<re.Match object; span=(0, 4), match='abbb'>\n",
      "--------------------------------------------------\n",
      "None\n",
      "<re.Match object; span=(0, 2), match='ab'>\n",
      "<re.Match object; span=(0, 4), match='abbb'>\n",
      "--------------------------------------------------\n",
      "<re.Match object; span=(0, 1), match='a'>\n",
      "<re.Match object; span=(0, 2), match='ab'>\n",
      "<re.Match object; span=(0, 2), match='ab'>\n",
      "--------------------------------------------------\n",
      "None\n",
      "<re.Match object; span=(0, 2), match='aa'>\n",
      "<re.Match object; span=(0, 4), match='aaaa'>\n",
      "<re.Match object; span=(0, 2), match='aa'>\n",
      "<re.Match object; span=(0, 2), match='aa'>\n",
      "--------------------------------------------------\n",
      "<re.Match object; span=(0, 19), match='Hello amazing World'>\n",
      "<re.Match object; span=(0, 16), match='cat and then dog'>\n"
     ]
    }
   ],
   "source": [
    "# * \n",
    "\n",
    "# Zero or more 'b' after 'a'\n",
    "pattern = r'ab*'\n",
    "\n",
    "print(re.search(pattern, 'a'))\n",
    "print(re.search(pattern, 'ab'))\n",
    "print(re.search(pattern, 'abbb'))\n",
    "print(\"--\"*25)\n",
    "\n",
    "# +\n",
    "# One or more 'b' after 'a'\n",
    "pattern = r'ab+'\n",
    "\n",
    "print(re.search(pattern, 'a')) # Match Not Found\n",
    "print(re.search(pattern, 'ab'))\n",
    "print(re.search(pattern, 'abbb'))\n",
    "print(\"--\"*25)\n",
    "\n",
    "# Zero or one 'b' after 'a'\n",
    "pattern = r'ab?'\n",
    "print(re.search(pattern, 'a'))\n",
    "print(re.search(pattern, 'ab'))\n",
    "print(re.search(pattern, 'abbb'))\n",
    "print(\"--\"*25)\n",
    "\n",
    "\n",
    "# {m,n} {m} {m,} {,n}\n",
    "print(re.search(r'a{2,4}', 'a')) # No Match\n",
    "print(re.search(r'a{2,4}', 'aa')) # Match found (\"aa\")\n",
    "print(re.search(r'a{2,4}', 'aaaaa')) # # Match found (\"aaaa\")\n",
    "print(re.search(r'a{,2}', 'aaaaa')) # # Match found (\"aaaa\")\n",
    "print(re.search(r'a{2}', 'aaaaa')) # # Match found (\"aaaa\")\n",
    "print(\"--\"*25)\n",
    "\n",
    "# pat1.*pat2\n",
    "print(re.search(r'Hello.*World', 'Hello amazing World')) # Match Found\n",
    "\n",
    "\n",
    "# pat1.*pat2|pat2.*pat1\n",
    "print(re.search(r'cat.*dog|dog.*cat', 'cat and then dog'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Character Classes**\n",
    "\n",
    "- `[aeiou]` : Matches any one of the specified characters (vowels).\n",
    "- `[^aeiou]` :  The ^ inverts the selection, so this matches any character except the specified ones (consonants in this case).\n",
    "- `[a-f]`: Matches any one character in the specified range.\n",
    "- `\\d` : Matches any digit, equivalent to [0-9].\n",
    "- `\\D` :  Matches any non-digit, equivalent to `[^0-9]` or `[^\\\\d]`.\n",
    "- `\\w` : Matches any word character (alphanumeric and underscore), equivalent to [a-zA-Z0-9_].\n",
    "- `\\W`: Matches any non-word character, equivalent to [^a-zA-Z0-9_] or `[^\\\\w]`.\n",
    "- `\\s`: Matches any whitespace character, including space, tab, newline, carriage return, form feed, and vertical tab. Equivalent to `[\\ \\t\\n\\r\\f\\v]`.\n",
    "- `\\S`: Matches any character that is not a whitespace character. Equivalent to `[^\\ \\t\\n\\r\\f\\v]` or `[^\\s]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', 'o', 'o']\n",
      "['h', 'l', 'l', ' ', 'w', 'r', 'l', 'd']\n",
      "['a', 'b', 'c', 'd', 'e', 'f']\n",
      "['1', '2', '3']\n",
      "['a', 'b', 'c', 'x', 'y', 'z']\n",
      "['h', 'e', 'l', 'l', 'o', '_', 'w', 'o', 'r', 'l', 'd', '1', '2', '3']\n",
      "[' ', '!']\n",
      "[' ', '\\t', '\\n']\n",
      "['h', 'e', 'l', 'l', 'o', 'w', 'o', 'r', 'l', 'd', '!', '1', '2', '3']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r'[aeiou]', 'hello world')) # Outputs: ['e', 'o', 'o']\n",
    "print(re.findall(r'[^aeiou]', 'hello world')) # ['h', 'l', 'l', ' ', 'w', 'r', 'l', 'd']\n",
    "print(re.findall(r'[a-f]', 'abcdefgxyz')) # ['a', 'b', 'c', 'd', 'e', 'f']\n",
    "print(re.findall(r'\\d', 'abc123xyz')) # ['1', '2', '3']\n",
    "print(re.findall(r'\\D', 'abc123xyz')) # ['a', 'b', 'c', 'x', 'y', 'z']\n",
    "print(re.findall(r'\\w', 'hello_world 123!')) # ['h', 'e', 'l', 'l', 'o', '_', 'w', 'o', 'r', 'l', 'd', '1', '2', '3']\n",
    "print(re.findall(r'\\W', 'hello_world 123!')) # [' ', '!']\n",
    "print(re.findall(r'\\s', 'hello world\\t123\\n'))  # [' ', '\\t', '\\n']\n",
    "print(re.findall(r'\\S', 'hello world!\\t123\\n')) # ['h', 'e', 'l', 'l', 'o', 'w', 'o', 'r', 'l', 'd', '!', '1', '2', '3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lookarounds**\n",
    "\n",
    "\n",
    "- `(?=pat)` : Positive Lookahead Assertion, Asserts that the given pattern matches after the current position.\n",
    "\n",
    "    ```python\n",
    "    # Match 'foo' only if it is followed by 'bar'\n",
    "    pattern = r'foo(?=bar)'\n",
    "    ```\n",
    "\n",
    "- `(?<=pat)` :  Positive Lookbehind Assertion, Asserts that the given pattern matches before the current position.\n",
    "\n",
    "    ```python \n",
    "    # Match 'bar' only if it is preceded by 'foo'\n",
    "    pattern = r'(?<=foo)bar'\n",
    "    ```\n",
    "\n",
    "- `(?!pat)` :  Negative Lookahead Assertion,  Asserts that the given pattern does not match after the current position.\n",
    "  \n",
    "    ```python\n",
    "    # Match 'foo' only if it is not followed by 'bar'\n",
    "    pattern = r'foo(?!bar)'\n",
    "\n",
    "    ```\n",
    "\n",
    "- `(?<!pat)` : Negative Lookbehind Assertion, Asserts that the given pattern does not match before the current position.\n",
    "\n",
    "    ```python\n",
    "    # Match 'bar' only if it is not preceded by 'foo'\n",
    "    pattern = r'(?<!foo)bar', \n",
    "\n",
    "    ```\n",
    "\n",
    "\n",
    "- `(?!pat1)(?=pat2)`: Multiple Assertions, \n",
    "  \n",
    "    ```python \n",
    "    # Match 'foo' only if it is not followed by 'bar' and followed by 'baz'\n",
    "    pattern = r'foo(?!bar)(?=baz)'\n",
    "\n",
    "\n",
    "    ```\n",
    "\n",
    "- `((?!pat).)*`: Negate a Grouping\n",
    "  ```python \n",
    "    # Match any sequence of characters that does not contain 'foo'\n",
    "    pattern = r'((?!foo).)*'\n",
    "\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 3), match='foo'>\n",
      "None\n",
      "<re.Match object; span=(3, 6), match='bar'>\n",
      "None\n",
      "None\n",
      "<re.Match object; span=(0, 3), match='foo'>\n",
      "None\n",
      "<re.Match object; span=(3, 6), match='bar'>\n",
      "None\n",
      "<re.Match object; span=(0, 3), match='foo'>\n",
      "<re.Match object; span=(0, 0), match=''>\n",
      "<re.Match object; span=(0, 6), match='barbaz'>\n"
     ]
    }
   ],
   "source": [
    "# Match 'foo' only if it is followed by 'bar'\n",
    "pattern = r'foo(?=bar)'\n",
    "print(re.search(pattern, 'foobar'))\n",
    "print(re.search(pattern, 'foobaz'))\n",
    "\n",
    "# Match 'bar' only if it is preceded by 'foo'\n",
    "pattern = r'(?<=foo)bar'\n",
    "print(re.search(pattern, 'foobar'))  # Match found: 'bar'\n",
    "print(re.search(pattern, 'bazbar'))  # No match\n",
    "\n",
    "\n",
    "# Match 'foo' only if it is not followed by 'bar'\n",
    "pattern = r'foo(?!bar)'\n",
    "print(re.search(pattern, 'foobar'))  # No match\n",
    "print(re.search(pattern, 'foobaz'))  # Match found: 'foo')\n",
    "\n",
    "# Match 'bar' only if it is not preceded by 'foo'\n",
    "pattern = r'(?<!foo)bar'\n",
    "print(re.search(pattern, 'foobar'))  # No match\n",
    "print(re.search(pattern, 'bazbar'))  # Match found: 'bar'\n",
    "\n",
    "# Match 'foo' only if it is not followed by 'bar' and followed by 'baz'\n",
    "pattern = r'foo(?!bar)(?=baz)'\n",
    "print(re.search(pattern, 'foobarbaz'))  # No match\n",
    "print(re.search(pattern, 'foobaz'))     # Match found: 'foo'\n",
    "\n",
    "\n",
    "# Match any sequence of characters that does not contain 'foo'\n",
    "pattern = r'((?!foo).)*'\n",
    "print(re.search(pattern, 'foobarbaz'))  # Matches '', stops before 'foo'\n",
    "print(re.search(pattern, 'barbaz'))     # Matches 'barbaz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Funcions**\n",
    "\n",
    "| Function        | Description                                                                                                                                                        |\n",
    "|-----------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| `re.search`     | Check if the given pattern is present anywhere in the input string. Output is a `re.Match` object, usable in conditional expressions. Prefer r-strings to define RE. Use byte pattern for byte input. Python also maintains a small cache of recent RE. |\n",
    "| `re.fullmatch`  | Ensures pattern matches the entire input string.                                                                                                                   |\n",
    "| `re.compile`    | Compile a pattern for reuse, outputs `re.Pattern` object.                                                                                                          |\n",
    "| `re.sub`        | Search and replace. `re.sub(r'pat', f, s)` uses function `f` with `re.Match` object as argument.                                                                   |\n",
    "| `re.escape`     | Automatically escape all metacharacters.                                                                                                                           |\n",
    "| `re.split`      | Split a string based on RE. Text matched by the groups will be part of the output. Portion matched by pattern outside group won't be in output.                    |\n",
    "| `re.findall`    | Returns all the matches as a list. If 1 capture group is used, only its matches are returned. If 1+ capture groups are used, each element will be a tuple of capture groups. Portion matched by pattern outside group won't be in output.            |\n",
    "| `re.finditer`   | Iterator with `re.Match` object for each match.                                                                                                                    |\n",
    "| `re.subn`       | Gives tuple of modified string and number of substitutions.                                                                                                        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "world\n",
      "hello crazy world\n",
      "hello world\n",
      "['123', '456']\n",
      "the dog sat on the dog\n",
      "hello\\.\\ how\\ are\\ you\\?\n",
      "['one', 'two', 'three', 'four']\n",
      "hello\n",
      "world\n",
      "('the dog sat on the dog', 2)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "######################### Search ###############################\n",
    "\n",
    "# Match 'hello' followed by any characters and then 'world'\n",
    "pattern = r'(^h\\w+).*(w\\w+)$'\n",
    "match = re.search(pattern, 'hello crazy world')\n",
    "\n",
    "if match:\n",
    "    print(match.group(1))  # Outputs: hello\n",
    "    print(match.group(2))  # Outputs: world\n",
    "    print(match.group())  # Outputs: world\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "######################### Full Match ###############################\n",
    "\n",
    "match = re.fullmatch(r'hello world', 'hello world')\n",
    "if match:\n",
    "    print(match.group())  # Outputs: hello world\n",
    "\n",
    "\n",
    "######################### Compile ###############################\n",
    "\n",
    "pattern = re.compile(r'\\d+')\n",
    "matches = pattern.findall('123 abc 456 def')\n",
    "print(matches)  # Outputs: ['123', '456']\n",
    "\n",
    "\n",
    "######################### Sub ###############################\n",
    "result = re.sub(r'cat', 'dog', 'the cat sat on the cat')\n",
    "print(result)  # Outputs: the dog sat on the dog\n",
    "\n",
    "\n",
    "######################### Escape ###############################\n",
    "escaped_string = re.escape('hello. how are you?')\n",
    "print(escaped_string)  # Outputs: hello\\. how are you\\?\n",
    "\n",
    "######################### Split ###############################\n",
    "result = re.split(r'\\d+', 'one1two2three3four')\n",
    "print(result)\n",
    "\n",
    "######################### Find Iter ###############################\n",
    "matches = re.finditer(r'\\b\\w+\\b', 'hello world')\n",
    "for match in matches:\n",
    "    print(match.group())  # Outputs: hello \\n world\n",
    "\n",
    "######################### subn ###############################\n",
    "result = re.subn(r'cat', 'dog', 'the cat sat on the cat')\n",
    "print(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# practice questios: \n",
    "\n",
    "sample_string = \"The quick brown fox jumps over the lazy dog. Email: test.email+alex@leetcode.com or contact_us@company.org. Visit https://www.example.com/path?query=123&lang=en or http://short.url for more info. Today is 2024-06-21. Call us at (123) 456-7890 or 123-456-7890. Hex color codes: #1a2b3c, #FFF, #123456. IPv4 addresses: 192.168.1.1, 255.255.255.255. Use password P@ssw0rd123! and reset it by 12:34 PM.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.email+alex@leetcode.com\n",
      "contact_us@company.org\n"
     ]
    }
   ],
   "source": [
    "# find all email address\n",
    "pattern = r'((www\\.)?[a-z0-9._%+-]+@[a-z0-9.-]+\\.[a-zA-Z]{2,})'\n",
    "match = re.finditer(pattern,sample_string )\n",
    "for m in match:\n",
    "    print(m.group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.example.com/path?query=123&lang=en\n",
      "http://short.url\n"
     ]
    }
   ],
   "source": [
    "# url \n",
    "pattern = r\"https?:\\/\\/(www\\.)?[^\\s\\/]+\\.[a-zA-Z]{2,}(\\/[\\S]+)?\"\n",
    "match = re.finditer(pattern,sample_string )\n",
    "for m in match:\n",
    "    print(m.group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (123) 456-7890\n",
      "+1 (234) 567-8901\n",
      " 345.678.9012\n",
      " 4374383992\n"
     ]
    }
   ],
   "source": [
    "# phone number \n",
    "\n",
    "sample_text = \"Here are some phone numbers: (123) 456-7890, +1 (234) 567-8901, 345.678.9012 4374383992\"\n",
    "pattern = r\"(\\+\\d{1,3})?[-.\\s]?\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}\"\n",
    "match = re.finditer(pattern,sample_text )\n",
    "for m in match:\n",
    "    print(m.group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['192.168.1.1', '255.255.255.255']\n",
      "['1.', '255.']\n"
     ]
    }
   ],
   "source": [
    "# IP address\n",
    "pattern = r'(?:\\d{1,3}\\.){3}\\d{1,3}'\n",
    "print(re.findall(pattern,sample_string))\n",
    "\n",
    "\n",
    "pattern = r'(\\d{1,3}\\.){3}\\d{1,3}'\n",
    "print(re.findall(pattern,sample_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Capturing Group (`()`)**: Captures the matched text and stores it for later use or extraction using functions like `re.findall`.\n",
    "- **Non-Capturing Group `(?:)`**: Matches the pattern enclosed in parentheses but does not capture or store the matched text. It is useful when you need to group elements for applying quantifiers or alternatives but don't need to extract them separately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('123', '4567'), ('', '8910')]\n",
      "['123-4567', '8910']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"123-4567 hello 8910\"\n",
    "\n",
    "# Capturing \n",
    "pattern = r\"(\\d{3})?-?(\\d{4})\"\n",
    "matches = re.findall(pattern, text)\n",
    "print(matches)  # Output: [('123', '4567'), ('', '8910')]\n",
    "\n",
    "\n",
    "# Non Capturing \n",
    "pattern = r\"(?:\\d{3})?-?(?:\\d{4})\"\n",
    "matches = re.findall(pattern, text)\n",
    "print(matches)  # Output: ['123-4567', '8910']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Turns arbitrary text into fixed-length vectors by counting how many times each word appears in the corpus. \n",
    "- This approach is very simple and flexible technique. It involves two things:\n",
    "  - A vocabulary of known words\n",
    "  - A measure of the presence of known words\n",
    "- The feature vector representing each will be sparse in nature as the words in each document will represent only a small subset of words out of all words (bag-of-words) present in entire set of document.\n",
    "\n",
    "\n",
    "Drawbacks: \n",
    "- The size of the vector increases with the size of the vocabulary which may cause sparsity.\n",
    "- Discarding the word order ignores the context and in turn meaning of the words in documents.\n",
    "- Cannot handle out of vocabulary (OOV) tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>guys</th>\n",
       "      <th>language</th>\n",
       "      <th>love</th>\n",
       "      <th>natural</th>\n",
       "      <th>need</th>\n",
       "      <th>on</th>\n",
       "      <th>processing</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>we</th>\n",
       "      <th>work</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   guys  language  love  natural  need  on  processing  the  to  we  work\n",
       "0     0         1     1        1     0   0           1    0   0   2     0\n",
       "1     1         1     0        1     1   1           1    1   1   1     1"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = [\"We love Natural Language Processing we.\", \"Guys, we need to work on the Natural Language Processing.\"]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "data = vectorizer.fit_transform(text)\n",
    "\n",
    "bow = pd.DataFrame(data.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'we': 9, 'love': 2, 'natural': 3, 'language': 1, 'processing': 6, 'guys': 0, 'need': 4, 'to': 8, 'work': 10, 'on': 5, 'the': 7}\n"
     ]
    }
   ],
   "source": [
    "# print the vocabulary \n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some important arguments in Count Vectorizer; \n",
    "ngram_range=(1,3) # N-Gram of length 1-3\n",
    "min_df=2 # Repeated atleast twice \n",
    "max_df=0.9 # Occurs in more than 90% of the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In bag of words all words in the text are treated equally and there’s no consideration that some words in the document are more important than others. \n",
    "- The TF-IDF technique aims to quantify the importance of a given word relative to other words in the document.\n",
    "- The main intuition behind TF-IDF is that if a word $w$ appears much time in the document $d_i$ but doesn’t appear in the rest of the document $d_j$ then word w must be of great importance to the document $di$.\n",
    "- The importance of $w$ should increase in proportion to its frequency in $d_i$, but at the same time, its importance should decrease in the proportion to other documents $d_j$.\n",
    "\n",
    "<u>Term Frequency:</u> \n",
    "\n",
    "- The TF score measures how often a word occurs in a document.\n",
    "\n",
    "$$\\mathrm{TF}\\left(t,d\\right)=\\frac{(\\text{Number of occurrences of term }t\\text{ in document }d)}{(\\text{Total number of terms in the document }d)}$$\n",
    "\n",
    "- In a given corpus, we can have different lengths of documents, and the occurrence of a word in a document may vary based on the length of the document.To normalize this, we divide the number of occurrences by the length of the document.\n",
    "\n",
    "<u>Inverse Document Frequency</u>\n",
    "\n",
    "- It measures the importance of a word across the corpus.\n",
    "  \n",
    "$$\n",
    "\\mathrm{IDF}\\left(t\\right)=\\log_e\\frac{(\\text{Total number of documents in the corpus})}{(\\text{Number of documents with term }t\\text{ in them })}\n",
    "$$\n",
    "\n",
    "- The Term frequency(TF) gives equal weight to all words, but this may not be useful as stopwords like is, am, are, etc., are not important, even they occur frequently.\n",
    "\n",
    "- To account for this IDF weights up the term that occurs less commonly in the corpus and weights down the words that occur very frequently in the corpus. \n",
    "\n",
    "Drawbacks: \n",
    "-  It is based on the bag of words (BOW) model, therefore it doesn’t capture the position in the text, semantics, etc.\n",
    "- Cannot handle out of vocabulary (OOV) tokens.\n",
    "- The feature vectors are sparse and high dimensional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: \n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*-0g0HFp9BKdgYSJOUhlKOA.png)\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:640/format:webp/1*ZXVW1MFpPGOgDne8spdawA.png)\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:828/format:webp/1*owBBWicmILMFw7gHvE5rzw.png)\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:828/format:webp/1*FCG4RqeoOIdttCVHh9qxcA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fourth</th>\n",
       "      <th>from</th>\n",
       "      <th>is</th>\n",
       "      <th>jupiter</th>\n",
       "      <th>largest</th>\n",
       "      <th>mars</th>\n",
       "      <th>planet</th>\n",
       "      <th>sun</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.379303</td>\n",
       "      <td>0.533098</td>\n",
       "      <td>0.533098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.379303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.379303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.376957</td>\n",
       "      <td>0.376957</td>\n",
       "      <td>0.268208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.376957</td>\n",
       "      <td>0.268208</td>\n",
       "      <td>0.376957</td>\n",
       "      <td>0.536416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     fourth      from        is   jupiter   largest      mars    planet  \\\n",
       "0  0.000000  0.000000  0.379303  0.533098  0.533098  0.000000  0.379303   \n",
       "1  0.376957  0.376957  0.268208  0.000000  0.000000  0.376957  0.268208   \n",
       "\n",
       "        sun       the  \n",
       "0  0.000000  0.379303  \n",
       "1  0.376957  0.536416  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text = [\"Jupiter is the largest planet\", \"Mars is the fourth planet from the sun\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "data = vectorizer.fit_transform(text)\n",
    "\n",
    "tfidf = pd.DataFrame(data.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'jupiter': 3, 'is': 2, 'the': 8, 'largest': 4, 'planet': 6, 'mars': 5, 'fourth': 0, 'from': 1, 'sun': 7}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some important arguments in TF-IDF Vectorizer; \n",
    "ngram_range=(1,3) # N-Gram of length 1-3\n",
    "min_df=2 # Repeated atleast twice \n",
    "max_df=0.9 # Occurs in more than 90% of the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Puncutation Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HelloArjanHow are you Fcker\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "my_string = \"Hello!Arjan!#^*(!*@)Ho*w are you F*cker?\"\n",
    "\n",
    "'''\n",
    "translate() : retuns string where each char is mapped to corresponding char in \n",
    "translation table. \n",
    "\n",
    "translation table is formed using maketrans()\n",
    "\n",
    "'''\n",
    "puncts = string.punctuation\n",
    "table = str.maketrans('','',puncts)\n",
    "clean_str = my_string.translate(table)\n",
    "print(clean_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WhiteSpace Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Hello Rajan Ghimire  \n",
      "Hello Rajan Ghimire\n"
     ]
    }
   ],
   "source": [
    "# Remove leading and ending white spaces \n",
    "\n",
    "input_string = \"\\t Hello Rajan Ghimire  \"\n",
    "print(input_string)\n",
    "print(input_string.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('love', 'VBP'), ('to', 'TO'), ('write', 'VB'), ('programming', 'VBG'), ('blogs.Those', 'JJ'), ('blogs', 'NNS'), ('are', 'VBP'), ('available', 'JJ'), ('at', 'IN'), ('my', 'PRP$'), ('portfolio', 'NN'), ('site', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from textblob import TextBlob\n",
    "\n",
    "# download \n",
    "# nltk.download(\"tagsets\")\n",
    "# nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "text = (\"I love to write programming blogs.\"\"Those blogs are available at my portfolio site.\")\n",
    "\n",
    "tagger = TextBlob(text)\n",
    "\n",
    "print(tagger.tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "# print the tags-set\n",
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named Entity Recognization\n",
    "\n",
    "(NER) seeks to locate and classify named entities in text into pre-defined categories such as the \n",
    "- names of persons, \n",
    "- organizations, \n",
    "- locations, \n",
    "- expressions of times, \n",
    "- quantities, \n",
    "- monetary values, \n",
    "- percentages, etc. \n",
    "\n",
    "NER is used in many fields in Natural Language Processing (NLP), and it can help answering many real-world questions:\n",
    "\n",
    "Which companies were mentioned in the news article?\n",
    "Were specified products mentioned in complaints or reviews?\n",
    "Does the tweet contain the name of a person? Does the tweet contain this person’s location?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download(\"maxent_ne_chunker\")\n",
    "# nltk.download(\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "my_string = \"Rajan Ghimire worked for Google and attended meeting in Kathmandu. I study in Lambton College in Toronto.\"\n",
    "\n",
    "# apply sentence tokenizer \n",
    "sentences = sent_tokenize(my_string)\n",
    "\n",
    "# apply word tokneizer \n",
    "words = [word_tokenize(x) for x in sentences]\n",
    "\n",
    "# apply pos tagger ? Why pos \n",
    "pos = [nltk.pos_tag(x) for x in words]\n",
    "\n",
    "# get ner chunks named entity : chunker to chunk the given list of tagged sentences\n",
    "chunked_sentences = nltk.ne_chunk_sents(pos, binary=True)\n",
    "\n",
    "# for sent in chunked_sentences:\n",
    "#     print(sent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NE Rajan/NNP Ghimire/NNP)\n",
      "(NE Google/NNP)\n",
      "(NE Kathmandu/NNP)\n",
      "(NE Lambton/NNP College/NNP)\n",
      "(NE Toronto/NNP)\n"
     ]
    }
   ],
   "source": [
    "# only print the named entities\n",
    "# use hasattr(), if NE chunk then its a nltk object and \n",
    "# in that object it is stored in 'label' varibale. First check if object has label \n",
    "# attribute and if present, check if label is NE\n",
    "for sent in chunked_sentences:\n",
    "    for chunk in sent:\n",
    "        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n",
    "            print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collocations\n",
    "\n",
    "-  It is a phrase consisting of more than one word but these words more commonly co-occur in a given context than its individual word parts.\n",
    "-  For example, the phrase ‘CT scan’ is more likely to co-occur than do ‘CT’ and ‘scan’ individually. ‘CT scan’ is also a meaningful phrase.\n",
    "-  **How do we make good selections for collocations?**\n",
    "-  Co-occurences may not be sufficient as phrases such as ‘of the’ may co-occur frequently, but are not meaningful.\n",
    "  \n",
    "Methods to filter out the most meaningful collocations: \n",
    "- frequency counting, \n",
    "- Pointwise Mutual Information (PMI), and \n",
    "- hypothesis testing (t-test and chi-square).\n",
    "\n",
    "Some uses for collocation identification are:\n",
    "-  Keyword extraction\n",
    "-  Bigrams/Trigrams can be concatenated (e.g. social media -> social_media) and counted as one word to improve insights analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[']', 'cameraman', ':', 'christ', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package webtext to /home/rjn/nltk_data...\n",
      "[nltk_data]   Package webtext is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# lets use a sample dataset provided by NLTK. \n",
    "# Load the data and convert the data into lowercase\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('webtext')\n",
    "\n",
    "from nltk.corpus import webtext\n",
    "\n",
    "words = [w.lower() for w in webtext.words('grail.txt')]\n",
    "\n",
    "print(words[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 1: Counting frequencies of adjacent words with part of speech filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\"'\", 's'), ('arthur', ':'), ('#', '1'), (\"'\", 't'), ('villager', '#')]\n",
      "[('[', 'boom', ']'), ('[', 'singing', ']'), ('[', 'music', ']'), ('[', 'clang', ']'), ('.', 'arthur', ':')]\n"
     ]
    }
   ],
   "source": [
    "# The simplest method is to rank the most frequent bigrams or trigrams:\n",
    "\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "from nltk.collocations import BigramAssocMeasures, TrigramAssocMeasures\n",
    "\n",
    "##################Bigram##################\n",
    "\n",
    "# Create a BigramCollocationFinder object\n",
    "bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "\n",
    "# Apply frequency filter to remove infrequent bigrams\n",
    "bigram_finder.apply_freq_filter(5)\n",
    "\n",
    "# filter bigram using likelihood ratio\n",
    "filtered = bigram_finder.nbest(score_fn=BigramAssocMeasures.likelihood_ratio, n=5)\n",
    "\n",
    "print(filtered)\n",
    "\n",
    "\n",
    "##################Trigram##################\n",
    "\n",
    "# Create a TrigramCollocationFinder object\n",
    "trigram_finder = TrigramCollocationFinder.from_words(words)\n",
    "\n",
    "# Apply frequency filter to remove infrequent trigrams\n",
    "trigram_finder.apply_freq_filter(5)\n",
    "\n",
    "# filter trigram using likelihood ratio\n",
    "filtered = trigram_finder.nbest(score_fn=TrigramAssocMeasures.likelihood_ratio, n=5)\n",
    "\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, a common issue with this is adjacent spaces, stop words, articles, prepositions or pronouns are common and are not meaningful. Lets apply text cleaning and see the reults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('black', 'knight'), ('clop', 'clop'), ('head', 'knight'), ('mumble', 'mumble'), ('squeak', 'squeak')]\n",
      "[('clop', 'clop', 'clop'), ('mumble', 'mumble', 'mumble'), ('squeak', 'squeak', 'squeak'), ('saw', 'saw', 'saw'), ('pie', 'iesu', 'domine')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopsets = set(stopwords.words('english'))\n",
    "\n",
    "# if len < 3, remove else remove stopwords\n",
    "filter_stop = lambda x : len(x) < 3 or x in stopsets\n",
    "\n",
    "\n",
    "##################Bigram##################\n",
    "\n",
    "# Create a BigramCollocationFinder object\n",
    "bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "\n",
    "# apply text cleaner \n",
    "bigram_finder.apply_word_filter(fn=filter_stop)\n",
    "\n",
    "# Apply frequency filter to remove infrequent bigrams\n",
    "bigram_finder.apply_freq_filter(5)\n",
    "\n",
    "# filter bigram using likelihood ratio\n",
    "filtered = bigram_finder.nbest(score_fn=BigramAssocMeasures.likelihood_ratio, n=5)\n",
    "\n",
    "print(filtered)\n",
    "\n",
    "\n",
    "\n",
    "##################Trigram##################\n",
    "\n",
    "# Create a TrigramCollocationFinder object\n",
    "trigram_finder = TrigramCollocationFinder.from_words(words)\n",
    "\n",
    "# Apply frequency filter to remove infrequent bigrams\n",
    "trigram_finder.apply_word_filter(fn=filter_stop)\n",
    "\n",
    "# Apply frequency filter to remove infrequent trigrams\n",
    "trigram_finder.apply_freq_filter(5)\n",
    "\n",
    "# filter trigram using likelihood ratio\n",
    "filtered = trigram_finder.nbest(score_fn=TrigramAssocMeasures.likelihood_ratio, n=5)\n",
    "\n",
    "print(filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Working Mechanims**\n",
    "\n",
    "Words -> BigramCollocationFinder/TrigramCollocationFinder -> create frequencey distrbution -> apply filtering (word eliminator, frequency filter) -> apply generic sorting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trigram</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(squeak, squeak, squeak)</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(saw, saw, saw)</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(clop, clop, clop)</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(pie, iesu, domine)</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(mumble, mumble, mumble)</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    trigram  freq\n",
       "6  (squeak, squeak, squeak)    15\n",
       "5           (saw, saw, saw)    14\n",
       "0        (clop, clop, clop)    13\n",
       "2       (pie, iesu, domine)    10\n",
       "9  (mumble, mumble, mumble)    10"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_freq = trigram_finder.ngram_fd.items()\n",
    "trigramFreqTable = pd.DataFrame(list(trigram_freq), columns=['trigram','freq']).sort_values(by='freq', ascending=False)\n",
    "\n",
    "trigramFreqTable.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method 2: Pointwise Mutual Information (PMI)\n",
    "Bigram: \n",
    "\n",
    "$$PMI(w^1,w^2)=log_2\\frac{P(w^1,w^2)}{P(w^1)P(w^2)}$$\n",
    "\n",
    "Trigram:\n",
    "\n",
    "$$PMI(w^1,w^2,w^3)=log_2\\frac{P(w^1,w^2,w^3)}{P(w^1)P(w^2)P(w^3)}$$\n",
    "\n",
    "- The main intuition is that it measures how much more likely the words co-occur than if they were independent.\n",
    "- However, it is very sensitive to rare combination of words. \n",
    "- For example, if a random bigram ‘abc xyz’ appears, and neither ‘abc’ nor ‘xyz’ appeared anywhere else in the text, ‘abc xyz’ will be identified as highly significant bigram when it could just be a random misspelling or a phrase too rare to generalize as a bigram. Therefore, this method is often used with a frequency filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('dramatic', 'chord'), ('dona', 'eis'), ('eis', 'requiem'), ('hand', 'grenade'), ('angels', 'sing')]\n",
      "[('dona', 'eis', 'requiem'), ('pie', 'iesu', 'domine'), ('saw', 'saw', 'saw'), ('clap', 'clap', 'clap'), ('squeak', 'squeak', 'squeak')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trigram</th>\n",
       "      <th>PMI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(dona, eis, requiem)</td>\n",
       "      <td>22.100888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(pie, iesu, domine)</td>\n",
       "      <td>21.457032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(saw, saw, saw)</td>\n",
       "      <td>19.645854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(clap, clap, clap)</td>\n",
       "      <td>18.702144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(squeak, squeak, squeak)</td>\n",
       "      <td>17.433835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(heh, heh, heh)</td>\n",
       "      <td>17.044521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(mumble, mumble, mumble)</td>\n",
       "      <td>16.702144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(clop, clop, clop)</td>\n",
       "      <td>15.945121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(brave, sir, robin)</td>\n",
       "      <td>14.613550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(boom, boom, boom)</td>\n",
       "      <td>13.947257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(king, arthur, music)</td>\n",
       "      <td>13.136998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     trigram        PMI\n",
       "0       (dona, eis, requiem)  22.100888\n",
       "1        (pie, iesu, domine)  21.457032\n",
       "2            (saw, saw, saw)  19.645854\n",
       "3         (clap, clap, clap)  18.702144\n",
       "4   (squeak, squeak, squeak)  17.433835\n",
       "5            (heh, heh, heh)  17.044521\n",
       "6   (mumble, mumble, mumble)  16.702144\n",
       "7         (clop, clop, clop)  15.945121\n",
       "8        (brave, sir, robin)  14.613550\n",
       "9         (boom, boom, boom)  13.947257\n",
       "10     (king, arthur, music)  13.136998"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################Bigram##################\n",
    "\n",
    "# Create a BigramCollocationFinder object\n",
    "bigram_finder = BigramCollocationFinder.from_words(words)\n",
    "\n",
    "# apply text cleaner \n",
    "bigram_finder.apply_word_filter(fn=filter_stop)\n",
    "\n",
    "# Apply frequency filter to remove infrequent bigrams\n",
    "bigram_finder.apply_freq_filter(5)\n",
    "\n",
    "# filter bigram using PMI\n",
    "filtered = bigram_finder.nbest(score_fn=BigramAssocMeasures.pmi, n=5)\n",
    "\n",
    "print(filtered)\n",
    "\n",
    "\n",
    "\n",
    "##################Trigram##################\n",
    "\n",
    "# Create a TrigramCollocationFinder object\n",
    "trigram_finder = TrigramCollocationFinder.from_words(words)\n",
    "\n",
    "# Apply frequency filter to remove infrequent bigrams\n",
    "trigram_finder.apply_word_filter(fn=filter_stop)\n",
    "\n",
    "# Apply frequency filter to remove infrequent trigrams\n",
    "trigram_finder.apply_freq_filter(5)\n",
    "\n",
    "# filter trigram using PMI\n",
    "filtered = trigram_finder.nbest(score_fn=TrigramAssocMeasures.pmi, n=5)\n",
    "\n",
    "print(filtered)\n",
    "\n",
    "\n",
    "pd.DataFrame(list(trigram_finder.score_ngrams(TrigramAssocMeasures.pmi)), columns=['trigram','PMI']).sort_values(by='PMI', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synonym and Antonym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Synonyms and Antonyms are part of the WordNet. WordNet is a large lexical database for the English language.\n",
    "- We use synsets to extract the synonyms and antonyms. \n",
    "- Synset: It is also called as synonym set or collection of synonym words\n",
    "\n",
    "Wordnet: \n",
    "- WordNet is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of cognitive synonyms (synsets), each expressing a distinct concept. \n",
    "- Synsets are interlinked by means of `conceptual-semantic` and `lexical relations`.\n",
    "- WordNet superficially resembles a thesaurus, in that it groups words together based on their meanings. But there are some differences: \n",
    "  \n",
    "  - WordNet connects specific meanings of words, not just the words themselves. This means that words that are related in WordNet are clearly defined by their meanings, avoiding confusion about what each word means.\n",
    "  - WordNet shows the relationships between words, like whether they are synonyms, antonyms, or related in other ways. In contrast, a thesaurus groups words together based only on similar meanings, without showing how they are related.\n",
    "\n",
    "**Synset components:**\n",
    "- `<lemma>` is the word’s morphological stem\n",
    "- `<pos>` is one of the module attributes ADJ, ADJ_SAT, ADV, NOUN or VERB\n",
    "  - n    NOUN\n",
    "  - v    VERB\n",
    "  - a    ADJECTIVE\n",
    "  - s    ADJECTIVE SATELLITE\n",
    "  - r    ADVERB \n",
    "\n",
    "- `<number>` is the sense number, counting from 0. This is used to disambiguate word meanings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('good.n.01'), Synset('good.n.02'), Synset('good.n.03'), Synset('commodity.n.01'), Synset('good.a.01'), Synset('full.s.06'), Synset('good.a.03'), Synset('estimable.s.02'), Synset('beneficial.s.01'), Synset('good.s.06'), Synset('good.s.07'), Synset('adept.s.01'), Synset('good.s.09'), Synset('dear.s.02'), Synset('dependable.s.04'), Synset('good.s.12'), Synset('good.s.13'), Synset('effective.s.04'), Synset('good.s.15'), Synset('good.s.16'), Synset('good.s.17'), Synset('good.s.18'), Synset('good.s.19'), Synset('good.s.20'), Synset('good.s.21'), Synset('well.r.01'), Synset('thoroughly.r.02')]\n",
      "[Lemma('good.n.02.good'), Lemma('good.n.02.goodness')]\n",
      "good.n.02\n",
      "moral excellence or admirableness\n",
      "['there is much good to be found in people']\n",
      "good\n",
      "[Lemma('evil.n.03.evil')]\n",
      "evil\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "syns = wordnet.synsets(\"good\")\n",
    "print(syns)\n",
    "print(syns[1].lemmas())\n",
    "print(syns[1].name())\n",
    "print(syns[1].definition())\n",
    "print(syns[1].examples())\n",
    "print(syns[1].lemmas()[0].name())\n",
    "\n",
    "# check if antonym\n",
    "print(syns[1].lemmas()[0].antonyms())\n",
    "print(syns[1].lemmas()[0].antonyms()[-1].name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_synonym_antonym(word):\n",
    "    \n",
    "    synonym = []\n",
    "    antonym = []\n",
    "    \n",
    "    for synset_object in wordnet.synsets(word):\n",
    "        for lemma in synset_object.lemmas():\n",
    "            synonym.append(lemma.name())\n",
    "            \n",
    "            if lemma.antonyms():\n",
    "                for ant in lemma.antonyms():\n",
    "                    antonym.append(ant.name())\n",
    "                    \n",
    "    \n",
    "    return set(synonym), set(antonym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Hades',\n",
       "  'Hell',\n",
       "  'Inferno',\n",
       "  'Scheol',\n",
       "  'blaze',\n",
       "  'hell',\n",
       "  'hell_on_earth',\n",
       "  'hellhole',\n",
       "  'infernal_region',\n",
       "  'inferno',\n",
       "  'nether_region',\n",
       "  'netherworld',\n",
       "  'perdition',\n",
       "  'pit',\n",
       "  'sin',\n",
       "  'snake_pit',\n",
       "  'the_pits',\n",
       "  'underworld'},\n",
       " {'Heaven'})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_synonym_antonym(\"hell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix word lengthening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "def remove_lengthening(text):\n",
    "    pattern = re.compile(r\"(.)\\1+\")\n",
    "    result = pattern.sub(r\"\\1\\1\", text)\n",
    "    if len(result) > 1 and result[-1] == result[-2]:\n",
    "        result = result[:-1]\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(remove_lengthening('Hellllloooooo'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spell Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "message\n",
      "car\n"
     ]
    }
   ],
   "source": [
    "from autocorrect import Speller\n",
    "spell = Speller(lang=\"en\")\n",
    "print(spell(\"mussage\"))\n",
    "print(spell(\"caaar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pattern.en import suggest\n",
    "print(suggest(\"mussage\"))\n",
    "print(suggest(\"survice\"))\n",
    "print(suggest(\"hte\"))\n",
    "print(suggest(\"caaar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
