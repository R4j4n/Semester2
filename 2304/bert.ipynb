{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rjn/.conda/envs/main_env/lib/python3.12/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/home/rjn/.conda/envs/main_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f667c045df0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random \n",
    "import torch \n",
    "from transformers import BertTokenizer, BertModel \n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "seed = 42 \n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): \n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The AIMT is a fantastic program at the lambton college.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rjn/.conda/envs/main_env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:2717: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer(text, return_tensors='pt', padding= True, truncation=False, max_length=64)\n",
    "input_ids = encoding[\"input_ids\"]\n",
    "attention_mask = encoding[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "# model.eval()\n",
    "\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    word_embedding = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of word embeddings: torch.Size([1, 15, 768])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of word embeddings: {word_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Text : the aimt is a fantastic program at the lambton college.\n",
      "tokenized text ['the', 'aim', '##t', 'is', 'a', 'fantastic', 'program', 'at', 'the', 'lamb', '##ton', 'college', '.']\n",
      "tensor([[  101,  1996,  6614,  2102,  2003,  1037, 10392,  2565,  2012,  1996,\n",
      "         12559,  2669,  2267,  1012,   102]])\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# the decoded text \n",
    "print(f\"Decoded Text : {decoded_text}\")\n",
    "\n",
    "tokenized_text = tokenizer.tokenize(decoded_text)\n",
    "\n",
    "print(f\"tokenized text {tokenized_text}\")\n",
    "\n",
    "encoded_text = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: the\n",
      "Embedding shape: torch.Size([768])\n",
      "\n",
      "\n",
      "Token: aim\n",
      "Embedding shape: torch.Size([768])\n",
      "\n",
      "\n",
      "Token: ##t\n",
      "Embedding shape: torch.Size([768])\n",
      "\n",
      "\n",
      "Token: is\n",
      "Embedding shape: torch.Size([768])\n",
      "\n",
      "\n",
      "Token: a\n",
      "Embedding shape: torch.Size([768])\n",
      "\n",
      "\n",
      "Token: fantastic\n",
      "Embedding shape: torch.Size([768])\n",
      "\n",
      "\n",
      "Token: program\n",
      "Embedding shape: torch.Size([768])\n",
      "\n",
      "\n",
      "Token: at\n",
      "Embedding shape: torch.Size([768])\n",
      "\n",
      "\n",
      "Token: the\n",
      "Embedding shape: torch.Size([768])\n",
      "\n",
      "\n",
      "Token: lamb\n",
      "Embedding shape: torch.Size([768])\n",
      "\n",
      "\n",
      "Token: ##ton\n",
      "Embedding shape: torch.Size([768])\n",
      "\n",
      "\n",
      "Token: college\n",
      "Embedding shape: torch.Size([768])\n",
      "\n",
      "\n",
      "Token: .\n",
      "Embedding shape: torch.Size([768])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for token, embedding in zip(tokenized_text, word_embedding[0]):\n",
    "    print(f\"Token: {token}\")\n",
    "    print(f\"Embedding shape: {embedding.shape}\")\n",
    "    print(f\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Embedding: \n",
      "tensor([[ 1.4300e-01,  9.2324e-02,  1.6427e-01,  1.3220e-01,  3.3594e-01,\n",
      "         -1.5824e-01,  2.7828e-01,  6.2212e-01, -2.4755e-01, -3.1975e-01,\n",
      "          2.3491e-01, -1.3382e-01,  4.8492e-01,  4.8614e-01,  8.1320e-02,\n",
      "         -1.4418e-01,  2.7171e-01, -4.3801e-03, -9.3331e-02, -1.1371e-01,\n",
      "         -1.9955e-01, -9.0681e-02, -1.7671e-01,  5.2152e-01,  4.3704e-01,\n",
      "          1.5830e-01,  2.9250e-01,  4.7411e-02, -2.0701e-01,  2.7105e-02,\n",
      "          1.5560e-01, -7.7934e-03, -2.7743e-01, -2.5712e-01,  2.9287e-02,\n",
      "          5.7685e-03, -3.7565e-01, -3.6692e-01, -2.2923e-01, -8.4029e-03,\n",
      "         -4.7351e-01, -4.7651e-01, -9.4768e-02,  5.1951e-02, -1.7419e-01,\n",
      "         -2.9894e-01, -2.7191e-01, -1.5192e-01,  1.3657e-01, -1.7907e-01,\n",
      "         -2.1145e-01,  2.3613e-01, -5.8511e-02, -2.5221e-01,  7.3377e-02,\n",
      "          4.8811e-01, -3.8451e-01, -2.4617e-01, -2.4480e-01, -7.6153e-02,\n",
      "         -1.4820e-01,  9.9156e-02, -3.0508e-01, -3.8623e-02,  3.4082e-02,\n",
      "          1.2756e-01,  1.5189e-01,  8.5021e-02, -6.5851e-01, -9.2416e-02,\n",
      "         -2.7680e-01, -3.8727e-01,  2.3586e-01, -1.6584e-01, -2.1027e-01,\n",
      "         -4.4095e-01, -1.0187e-01,  2.5775e-01,  3.9796e-01,  1.0892e-02,\n",
      "          2.9879e-01,  8.0939e-01, -4.5504e-02,  4.6068e-01,  3.6106e-01,\n",
      "          3.8861e-01, -9.3578e-02,  1.1525e-02, -1.8723e-01,  3.5727e-01,\n",
      "         -1.9665e-01, -8.4035e-02, -2.1907e-01,  5.2508e-02,  5.9187e-01,\n",
      "         -2.4791e-01, -7.6296e-02, -3.4311e-01, -1.2039e-01,  9.5624e-02,\n",
      "          1.0050e-01, -4.0999e-01,  1.4050e-01,  2.8207e-01, -6.0557e-02,\n",
      "         -1.8600e-01, -5.2721e-02, -1.4535e-01, -1.4090e-01, -2.6841e-01,\n",
      "         -6.4897e-02, -1.0055e-01,  9.1571e-02, -3.5662e-01, -1.4181e-01,\n",
      "          3.7110e-01,  2.3175e-01, -4.8043e-02, -4.1173e-01, -1.3456e-01,\n",
      "          3.0243e-02,  3.4013e-01,  3.1364e-01,  7.6970e-01, -1.2414e-01,\n",
      "          1.5173e-01, -1.0424e-01, -1.7607e-02, -9.8670e-02, -4.3869e-01,\n",
      "          1.0304e-01,  2.4981e-01,  5.1598e-01, -1.0326e-01, -6.0464e-02,\n",
      "          4.4252e-01, -8.4369e-02, -2.0523e-01, -5.1252e-01,  2.6347e-01,\n",
      "         -2.8581e-01,  7.7339e-02,  1.8280e-01, -3.0050e-01,  7.1603e-02,\n",
      "         -2.4702e-02, -1.6670e-01,  3.8186e-01,  1.2351e-01,  1.3749e-01,\n",
      "          1.6506e-01, -1.2561e-01, -4.3787e-01,  2.7438e-03, -3.3806e-01,\n",
      "         -1.3018e-01, -2.8919e-01,  6.5818e-01,  3.8267e-01,  1.4074e-01,\n",
      "          2.3362e-01,  2.2875e-02, -1.3675e-01,  6.2165e-02, -2.4580e-01,\n",
      "         -4.2311e-04, -9.4340e-02,  6.9168e-02, -4.9555e-02,  2.6442e-01,\n",
      "         -3.7907e-01, -2.3256e-01,  5.8925e-01, -2.6774e-01, -2.2285e-01,\n",
      "         -2.5260e-01,  3.4970e-01,  5.7730e-02,  3.8127e-02,  7.3863e-02,\n",
      "         -1.5416e+00,  3.0202e-01, -5.7189e-03, -3.0547e-01, -1.6534e-01,\n",
      "         -2.8011e-01, -6.2071e-02, -2.6758e-01, -2.9999e-01,  3.0328e-01,\n",
      "          3.3409e-02, -1.0201e-01, -1.9388e-01, -3.4731e-02,  3.8693e-01,\n",
      "         -3.4883e-01, -2.5856e-01, -7.1177e-03, -3.7083e-01,  2.6216e-01,\n",
      "          1.6405e-01, -1.8072e-01, -2.1718e-01, -1.4443e-01, -3.7919e-01,\n",
      "          1.4221e-01,  1.6984e-01,  1.5985e-01, -2.9206e-02,  2.5133e-01,\n",
      "         -1.2048e-01,  2.6909e-01,  2.9920e-02,  1.8971e-01, -1.2212e-02,\n",
      "         -1.2165e-01,  8.1018e-02, -2.7761e-01, -1.2347e-01, -3.3312e-01,\n",
      "          4.7718e-01, -1.1733e-01,  7.8427e-02,  2.4190e-01, -5.7870e-02,\n",
      "          4.2657e-01, -1.3351e-01, -3.2379e-01,  1.8228e-01,  2.5411e-01,\n",
      "          1.8994e-01, -2.8951e-01,  3.1253e-01, -4.7753e-03, -3.7616e-01,\n",
      "          2.0839e-01, -4.3189e-01, -2.9517e-02,  1.9648e-01, -2.4790e-02,\n",
      "         -1.8214e-02,  3.8760e-01,  8.1213e-02,  4.7819e-01,  3.4393e-02,\n",
      "          6.5393e-02,  3.9886e-01,  1.5896e-01,  7.7029e-02, -6.2584e-01,\n",
      "         -3.2814e-01, -5.1300e-01,  1.9306e-01, -1.3679e-01, -1.5267e-01,\n",
      "         -8.4640e-02,  1.8148e-01,  5.3052e-02,  1.9083e-01,  1.1063e-01,\n",
      "          3.6481e-01,  2.7653e-01, -4.0298e-02, -5.6307e-02, -2.1426e-01,\n",
      "         -3.1823e-01,  1.2621e-01, -1.6669e-03,  3.5802e-02,  1.6232e-01,\n",
      "         -2.4959e-01, -1.0405e-01,  1.1366e-01, -2.1532e-01, -4.1137e-01,\n",
      "         -5.0810e-02,  2.7031e-01,  9.7548e-02, -2.3164e-01, -3.4784e-01,\n",
      "         -5.4994e-02,  3.1666e-01, -4.4016e-01,  1.4215e-01,  5.1574e-02,\n",
      "          2.0930e-01,  1.9749e-01,  1.4316e-01,  1.3929e-01, -1.3769e-01,\n",
      "         -3.4574e-02,  1.5377e-01,  1.0242e-01, -3.3242e-01,  5.2465e-01,\n",
      "         -2.7325e-01,  2.9096e-01, -1.2302e-02, -7.9998e-02,  3.8613e-02,\n",
      "         -1.7072e-01,  1.5684e-01,  8.5750e-02,  1.9972e-01, -5.8447e-02,\n",
      "          3.6940e-01,  8.5935e-02, -2.2858e-01, -4.1937e+00, -4.4664e-01,\n",
      "         -1.1552e-01, -3.7576e-02,  3.9015e-01, -3.6330e-01, -4.9858e-02,\n",
      "         -2.3577e-01, -6.5594e-01, -1.5214e-01, -1.6957e-01, -3.7783e-01,\n",
      "          2.6647e-01,  3.7201e-01,  2.8417e-01,  3.6668e-01, -3.3234e-02,\n",
      "         -1.2732e-01, -9.1640e-02,  5.6064e-01,  2.2983e-01, -3.2518e-01,\n",
      "          7.1523e-03, -2.6701e-01,  3.0237e-01,  8.7089e-02, -5.5757e-01,\n",
      "         -1.2179e-01, -4.5061e-01, -2.2397e-01,  8.9389e-02,  2.4231e-02,\n",
      "          1.1482e-01,  2.8034e-01,  1.8660e-02, -1.4328e-01,  2.6078e-01,\n",
      "         -1.9296e-01, -2.0164e-01, -1.0385e-01,  1.8017e-01, -4.0388e-01,\n",
      "          2.1298e-01,  2.5101e-02,  1.4169e-01, -3.8638e-02, -1.3583e-02,\n",
      "         -2.5567e-01, -1.7297e-02,  9.8146e-02, -1.9336e-01,  8.0306e-02,\n",
      "         -9.5803e-02, -1.9927e-01, -4.9480e-01, -2.5556e-01,  3.2054e-01,\n",
      "          7.3321e-01,  1.5400e-01, -2.5518e-01,  2.4390e-01, -3.1584e-01,\n",
      "          1.3360e-01,  3.7719e-02, -2.2723e-01,  4.0208e-02, -5.1945e-01,\n",
      "         -6.7294e-02,  7.1968e-02,  8.1645e-02,  1.9314e-01, -1.7686e-01,\n",
      "         -4.7989e-01, -1.0967e+00, -9.6021e-02, -3.8639e-01,  1.4798e-01,\n",
      "          2.1293e-01,  1.5637e-01,  4.3393e-02, -2.4913e-01, -5.3919e-01,\n",
      "          4.0344e-01,  3.3986e-01,  2.9826e-01, -3.9651e-01, -6.6641e-02,\n",
      "          8.2807e-02,  1.4055e-02, -3.4780e-01,  3.3767e-01,  1.7676e-01,\n",
      "          1.2079e-01,  3.8751e-01, -8.7028e-02, -1.5017e-01,  1.9543e-02,\n",
      "         -2.3804e-01,  6.8533e-02,  6.2607e-03, -6.8179e-02, -9.6096e-02,\n",
      "          2.0797e-01,  4.4483e-01, -1.9322e-01, -3.6758e-02, -1.8875e-01,\n",
      "          8.1864e-02,  9.3590e-02,  3.3102e-01,  4.5142e-01, -6.4129e-01,\n",
      "          3.2598e-02,  2.4986e-03,  7.2058e-02, -1.8720e-02, -6.3793e-02,\n",
      "          5.7272e-01,  1.1437e-02,  1.9879e-01,  1.5491e-01,  6.0868e-01,\n",
      "          1.1877e-01,  1.7071e-01, -1.8902e-01, -1.6843e-01, -7.0007e-02,\n",
      "         -1.1521e-01, -4.6229e-01, -1.0142e-01,  2.1099e-01, -3.6080e-01,\n",
      "         -7.4701e-02,  1.8768e-01, -1.1672e-01,  1.9198e-01, -1.3937e-01,\n",
      "         -9.9290e-03,  5.7978e-02,  5.6795e-02,  1.9675e-01,  5.8680e-01,\n",
      "         -2.9416e-02,  2.3402e-01,  4.0566e-02,  4.7980e-01, -9.6703e-02,\n",
      "          3.3573e-01, -2.4766e-01, -3.3046e-03,  3.0875e-02, -2.3601e-01,\n",
      "          1.4929e-02, -3.8492e-01,  2.8728e-01,  5.7892e-02,  1.9672e-01,\n",
      "         -2.1569e-01, -2.1653e-02, -1.5421e-02, -4.6319e-02,  2.8433e-01,\n",
      "          3.4682e-03,  2.3233e-01,  6.3703e-02,  4.7416e-01,  1.4944e-01,\n",
      "         -3.0000e-01, -3.6121e-01,  1.2558e-01,  2.0763e-02, -1.3086e-01,\n",
      "         -4.6947e-02, -3.4655e-01,  3.5785e-02,  1.0493e-01, -5.8387e-03,\n",
      "         -1.0617e-01, -3.3145e-02,  6.6653e-02,  4.2908e-01, -2.8171e-01,\n",
      "          2.2208e-01,  3.3750e-02,  3.5680e-01,  1.5454e-01, -2.5870e-01,\n",
      "         -2.7069e-01,  3.7446e-01,  9.6626e-02, -1.3565e-01,  4.1515e-01,\n",
      "         -2.0814e-01, -2.3785e-02, -6.3199e-01, -3.7644e-01, -1.9558e-01,\n",
      "         -4.4894e-01,  4.7562e-01,  3.2331e-01,  4.0246e-01,  2.8665e-01,\n",
      "          8.9210e-02,  1.8199e-02, -4.9667e-02,  1.3096e-01,  2.1244e-01,\n",
      "          1.5400e-01,  5.8807e-02, -1.2300e-01, -2.4637e-01, -2.0787e-01,\n",
      "         -9.8207e-02,  3.7013e-01,  6.0340e-02, -1.1737e-01,  2.5108e-02,\n",
      "         -1.1753e-01, -2.7891e-01,  2.6289e-01, -4.4888e-01,  1.7525e-01,\n",
      "          3.8486e-01,  2.6155e-01, -4.9494e-01, -9.5048e-02,  1.9761e-01,\n",
      "         -2.6759e-01, -6.0184e-01, -1.4484e-01,  8.2937e-02, -3.4629e-01,\n",
      "         -1.7286e-01, -2.5018e-02,  5.7887e-02,  1.6187e-01,  1.6685e-01,\n",
      "         -1.5433e-01,  4.0311e-02,  1.6160e-02, -3.0382e-01, -1.1191e-01,\n",
      "         -1.2471e-02, -5.2634e-01, -2.8093e-01, -3.9731e-02,  9.1880e-02,\n",
      "          3.7230e-01, -3.4994e-01,  2.2194e-01, -3.8349e-02, -2.3516e-01,\n",
      "         -1.1747e-01, -3.4910e-01, -2.5486e-01, -8.5423e-02,  2.7714e-01,\n",
      "          2.8951e-03, -1.9720e-01, -1.7708e-01, -6.8098e-02, -1.9179e-01,\n",
      "         -3.8280e-01,  1.3111e-01, -1.8782e-01,  9.3747e-02, -2.0481e-03,\n",
      "          6.1632e-02,  1.8351e-01, -2.2497e-01, -6.8211e-02,  9.7369e-02,\n",
      "         -8.7547e-02, -1.1390e-01,  1.3985e-01, -1.6807e-01,  8.0296e-02,\n",
      "         -6.3661e-01,  1.0613e-01, -2.8636e-01,  2.7860e-01,  2.5776e-01,\n",
      "          1.2609e-01, -1.5084e-01,  2.9201e-01, -6.8247e-02, -4.6736e-01,\n",
      "          3.4684e-01, -9.9145e-03,  1.6213e-02,  1.0767e-01, -3.0937e-01,\n",
      "         -6.8976e-02,  1.2208e-01, -2.0171e-01, -1.6826e-02,  4.7763e-01,\n",
      "         -2.6063e-01,  3.2966e-01,  2.7199e-02,  1.7303e-01,  3.8739e-01,\n",
      "          3.7453e-01,  1.1433e-01, -4.1136e-01,  2.6086e-01, -2.7573e-02,\n",
      "         -1.6656e-01,  2.0873e-01,  3.6764e-01,  9.7325e-03, -1.3331e-01,\n",
      "          6.2856e-01,  6.3675e-01, -5.7477e-01, -5.4331e-03, -1.3470e-02,\n",
      "         -1.9518e-01,  2.1386e-01, -1.5902e-01, -1.3064e-01,  4.3390e-02,\n",
      "          5.6813e-02,  2.2577e-01,  1.6692e-01,  5.2653e-01, -2.7602e-01,\n",
      "         -4.6254e-01,  1.2132e-01,  2.6738e-01, -2.4768e-01,  3.3735e-01,\n",
      "         -2.4017e-01,  3.9060e-01, -6.4098e-02, -1.3916e-01,  2.3870e-01,\n",
      "         -4.8671e-02, -1.9104e-02, -1.8134e-02, -7.2217e-02,  1.5164e-01,\n",
      "          4.1137e-01,  3.7070e-01,  2.1655e-01,  5.9598e-02,  3.0973e-01,\n",
      "          1.7648e-01,  1.7839e-01,  1.4487e-01, -5.7946e-02,  5.2819e-02,\n",
      "          7.3390e-02, -1.0711e-01,  1.5876e-01,  2.5301e-01, -1.6257e-02,\n",
      "         -8.4343e-02, -2.0363e-01,  1.1911e-01,  2.3344e-01,  8.3870e-02,\n",
      "          2.9497e-01, -4.2816e-01,  1.2559e-01, -1.3247e-01,  4.1838e-01,\n",
      "         -2.5765e-01, -2.8041e-01,  1.0505e-01,  1.3705e-01,  1.4208e-01,\n",
      "          1.9359e-01, -1.4923e-01,  1.4380e-01,  5.0538e-01,  2.5930e-01,\n",
      "         -9.0295e-02,  4.2507e-02,  2.2128e-01, -2.8505e-02, -7.4540e-02,\n",
      "         -3.7799e-01,  8.5844e-03, -2.7102e-01, -5.6986e-01, -4.3448e-02,\n",
      "         -1.2288e-02, -3.7580e-01,  5.5541e-02, -7.8491e-03, -1.4309e-01,\n",
      "          3.6001e-01, -3.1363e-03,  3.2350e-01, -3.2785e-01,  2.0065e-01,\n",
      "         -2.2687e-03,  1.4727e-01,  1.7101e-01,  1.8376e-01, -9.5575e-02,\n",
      "         -4.3685e-01, -7.6637e-02, -2.8966e-01, -6.8660e-02,  2.0824e-01,\n",
      "          1.2025e-01, -1.7556e-01, -4.2192e-01,  1.1912e-01,  5.2117e-01,\n",
      "         -7.7075e-01, -1.4698e-02, -4.1644e-02,  4.4576e-01,  1.1165e-01,\n",
      "          1.8034e-01, -2.7576e-01,  1.2887e-01, -3.7512e-01, -2.1648e-01,\n",
      "          2.9316e-02,  4.9196e-01,  9.7122e-02,  2.0989e-01, -2.3280e-01,\n",
      "          5.1710e-01, -1.6340e-01, -1.5828e-01,  4.1954e-01,  1.7024e-02,\n",
      "          1.6486e-01,  3.4280e-01,  1.2359e-01, -4.5313e-01, -5.0091e-01,\n",
      "          2.4303e-01, -5.8818e-02, -1.6414e-01, -4.1845e-03, -7.2787e-02,\n",
      "          1.0951e-01, -1.8459e-01, -5.4381e-01, -1.7576e-01,  1.6678e-02,\n",
      "         -1.1326e-01, -1.9824e-01, -4.3621e-01, -1.0086e-01, -2.5871e-01,\n",
      "          4.3146e-02, -2.4413e-01, -2.5260e-01, -2.6941e-01,  1.0937e-01,\n",
      "         -2.6338e-02,  2.8138e-01,  2.3483e-02]])\n",
      "Shape of Sentence Embedding: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "sentence_embedding = word_embedding.mean(dim=1)\n",
    "\n",
    "print(\"Sentence Embedding: \")\n",
    "print(sentence_embedding)\n",
    "\n",
    "print(f\"Shape of Sentence Embedding: {sentence_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentence = \"The lambton college is a great place and AIMT program open doors to future oppturnities.\"\n",
    "\n",
    "# tokenize and encode \n",
    "example_encoding = tokenizer(example_sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "example_input_ids = example_encoding[\"input_ids\"]\n",
    "example_attention_masks = example_encoding[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    example_outputs = model(example_input_ids,attention_mask= example_attention_masks)\n",
    "    example_sentence_embedding = example_outputs.last_hidden_state.mean(dim=1)\n",
    "    \n",
    "similarity = cosine_similarity(sentence_embedding, example_sentence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the similarity was : [[0.8690177]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"the similarity was : {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
